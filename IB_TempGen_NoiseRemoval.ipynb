{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Python Code (inside)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Started Python Code (inside)')\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "import traceback\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')  # Use the 'Agg' backend for non-GUI operations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\n",
    "from sklearn.svm import SVC   #for Support Vector Machine (SVM) Algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n",
    "from sklearn.model_selection import train_test_split #to split the dataset for training and testing\n",
    "\n",
    "import mne\n",
    "from mne.stats import permutation_cluster_test\n",
    "from timeit import default_timer as timer \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, Conv2D, Conv3D, MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from confidenceinterval import roc_auc_score\n",
    "import copy\n",
    "#from tdqm import tdqm\n",
    "\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "import sys\n",
    "#sys.path.append('C:/GitHub/ML_models/Whether_Project/')\n",
    "#from custumBoost_matrix import CustomFeatureFabrique  \n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#import pydot, graphviz\n",
    "\n",
    "class Subject_Data:\n",
    "    def __init__(self, subject, num_splits = 5, lab = 'Reed', directory = 'n/a',new = 1,folder_path = os.getcwd(),include_patterns = [''],exclude_patterns=['']):\n",
    "        self.num_splits = num_splits\n",
    "        self.lab = lab\n",
    "        self.directory = directory\n",
    "        self.subject = subject\n",
    "        self.data = []\n",
    "        self.mask = []\n",
    "        self.folder_path = folder_path\n",
    "        self.include_patterns = include_patterns\n",
    "        self.exclude_patterns = exclude_patterns\n",
    "        self.num_splits = num_splits\n",
    "        \n",
    "        self.new = new\n",
    "        self.load_and_concatenate_specific_pickles()\n",
    "        #self.load_data()\n",
    "\n",
    "    def print_all_info(self):\n",
    "        for attr_name, attr_value in self.__dict__.items():\n",
    "            print(f'{attr_name}: {attr_value}')\n",
    "\n",
    "    def start_data(self):\n",
    "        self.mat_files = glob.glob(os.path.join(self.final_folders[0], \"*.mat\"))\n",
    "        self.num_files = len(self.mat_files)\n",
    "        #loaded = 0\n",
    "        if self.new == 0:\n",
    "            print(\"Data is already loaded.\")\n",
    "            [self.experiment,self.phase,self.stimulus,self.split] = self.mask\n",
    "            self.data = np.array(self.data)\n",
    "            #num_splits = len(folders_list)\n",
    "            self.num_splits = 5\n",
    "        else:\n",
    "            # Get a list of all .mat files in the folder\n",
    "            print(\"Creating empty variables to load data.\")\n",
    "            self.mat_files = glob.glob(os.path.join(self.final_folders[0][0], \"*.mat\"))\n",
    "\n",
    "            #num_splits = len(folders_list)\n",
    "            self.num_splits = 5\n",
    "\n",
    "            # Preallocate arrays to the correct length\n",
    "            \n",
    "            if 'cha'  in self.lab or 'tel' in self.lab:\n",
    "                print(f'lab is {self.lab} so file is 64 by 531')\n",
    "                self.data = np.empty((self.num_files, 64, 519))  # Replace 'height' and 'width' with your actual data dimensions\n",
    "            else:\n",
    "                print(f'lab is {self.lab} so file is 59 by 519')\n",
    "                self.data = np.empty((self.num_files, 59, 519))\n",
    "\n",
    "            self.phase = [None] * self.num_files\n",
    "            self.stimulus = [None] * self.num_files\n",
    "            self.experiment = [None] * self.num_files\n",
    "            self.split = [None] * self.num_files\n",
    "                \n",
    "            # make new dataset!\n",
    "            #self.create_new_dataset(1)\n",
    "            self.load_data()\n",
    "        return self\n",
    "\n",
    "        # THIS IS FOR THE NEW PARADIGM\n",
    "    def load_data(self):\n",
    "\n",
    "        import pickle\n",
    "        with open('IB_eeg_data.pkl', 'rb') as f:\n",
    "            epochs, triggers, groups, phase, stim, subj  = pickle.load(f)\n",
    "        self.data = epochs.transpose(2,0,1)\n",
    "        self.groups = groups\n",
    "        self.phase = phase\n",
    "        self.stimulus = stim\n",
    "        self.subject_id = subj\n",
    "        self.triggers = triggers\n",
    "        self.experiment = [\"IB\"] * phase.shape[0] # THIS WILL NEED TO BE MODIFIED ONCE ALL EXPs are added\n",
    "        self.split_data(self.num_splits)\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def load_and_concatenate_specific_pickles(self):\n",
    "        all_epochs = []\n",
    "        all_triggers = []\n",
    "        all_labels = []\n",
    "        all_phase = []\n",
    "        all_stimuli = []\n",
    "        all_subjects = []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\") and all(pattern in file for pattern in self.include_patterns) and not any(pattern in file for pattern in self.exclude_patterns):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    print(f'Loading file {filepath}')\n",
    "                    with open(filepath, 'rb') as f:\n",
    "                        epochs, triggers, labels, phase, stimuli, subjects = pickle.load(f)\n",
    "                        epochs = epochs[:,500:1000,:]\n",
    "                        all_epochs.append(epochs)\n",
    "                        all_triggers.extend(triggers)\n",
    "                        all_labels.extend(labels)\n",
    "                        all_phase.extend(phase)\n",
    "                        all_stimuli.extend(stimuli)\n",
    "                        all_subjects.extend(subjects)\n",
    "\n",
    "        # Concatenate lists into numpy arrays\n",
    "        self.data = np.concatenate(all_epochs, axis=2).transpose(2, 0, 1)  # Assuming epochs are of shape (n_channels, n_times, n_epochs)\n",
    "        self.triggers = np.array(all_triggers)  # Shape: (total_epochs,)\n",
    "        self.groups = np.array(all_labels)  # Shape: (total_epochs,)\n",
    "        self.phase = np.array(all_phase)  # Shape: (total_epochs,)\n",
    "        self.stimulus = np.array(all_stimuli)  # Shape: (total_epochs,)\n",
    "        self.subject_id = np.array(all_subjects)  # Shape: (total_epochs,)\n",
    "        self.experiment = [\"IB\"] * self.phase.shape[0]  # THIS WILL NEED TO BE MODIFIED ONCE ALL EXPs are added\n",
    "\n",
    "        self.split_data(self.num_splits)\n",
    "        return self\n",
    "\n",
    "        # # Example usage\n",
    "        # folder_path = 'your/folder/path'\n",
    "        # all_epochs, all_triggers, all_labels, all_phase, all_stimuli, all_subjects = load_and_concatenate_pickles(folder_path)\n",
    "\n",
    "    def filter_data(self, variable_name, values, include=True):\n",
    "\n",
    "        print(f'Filtering Data based on f{variable_name}')\n",
    "        # Get the variable to filter on\n",
    "        variable = getattr(self, variable_name)\n",
    "\n",
    "        # Convert to NumPy array for consistency\n",
    "        variable = np.array(variable)\n",
    "        \n",
    "        # Create a boolean mask\n",
    "        if include:\n",
    "            mask = np.isin(variable, values)\n",
    "        else:\n",
    "            mask = ~np.isin(variable, values)\n",
    "\n",
    "        # Apply the mask to filter the data\n",
    "        self.data = self.data[mask, ...]\n",
    "        self.groups = self.groups[mask]\n",
    "        self.phase = np.array(self.phase)[mask]\n",
    "        self.stimulus = np.array(self.stimulus)[mask]\n",
    "        self.subject_id = np.array(self.subject_id)[mask]\n",
    "        self.triggers = np.array(self.triggers)[mask]\n",
    "        self.experiment  = np.array(self.experiment)[mask]\n",
    "        self.split = np.array(self.split)[mask]\n",
    "        return self\n",
    "\n",
    "    def split_data(self, num_of_splits):\n",
    "        # Generate random splits for the participants\n",
    "        unique_subjects = np.unique(self.subject_id)\n",
    "        splits = np.random.randint(1, num_of_splits + 1, size=len(unique_subjects))\n",
    "\n",
    "        # Create a dictionary to map each subject to their assigned split\n",
    "        subject_to_split = {subject: split for subject, split in zip(unique_subjects, splits)}\n",
    "\n",
    "        # Assign splits to each participant and create the 'test'/'train' arrays\n",
    "        subject_indices = np.array([subject_to_split[subj] for subj in self.subject_id]) - 1\n",
    "        split_matrix = np.tile(['train'], (len(self.subject_id), num_of_splits))\n",
    "        split_matrix[np.arange(len(self.subject_id)), subject_indices] = 'test'\n",
    "\n",
    "        #print(split_matrix)\n",
    "\n",
    "        self.split = split_matrix.tolist()\n",
    "        \n",
    "    def split_data2(self, num_of_splits=5):\n",
    "        num_samples = len(self.subject_id)\n",
    "        #print(num_samples)\n",
    "\n",
    "        # Randomly assign each data point to one of the splits\n",
    "        random_splits = np.random.randint(1, num_of_splits + 1, size=num_samples)\n",
    "\n",
    "        # Initialize split matrix with all 'train'\n",
    "        split_matrix = np.tile('train', (num_samples, num_of_splits))\n",
    "\n",
    "        # Assign 'test' based on the random splits\n",
    "        for i in range(1, num_of_splits + 1):\n",
    "            split_matrix[random_splits == i, i - 1] = 'test'\n",
    "\n",
    "        self.split = split_matrix.tolist()\n",
    "\n",
    "    def split_data_by_groups(self, num_of_splits):\n",
    "        # Separate unique subjects into two groups\n",
    "        unique_subjects = np.unique(self.subj)\n",
    "        group_0 = [subj for subj in unique_subjects if self.groups[np.where(self.subj == subj)[0][0]] == 0]\n",
    "        group_1 = [subj for subj in unique_subjects if self.groups[np.where(self.subj == subj)[0][0]] == 1]\n",
    "\n",
    "        print(group_0)\n",
    "        \n",
    "        # Shuffle the subjects within each group\n",
    "        np.random.shuffle(group_0)\n",
    "        np.random.shuffle(group_1)\n",
    "        \n",
    "        # Determine the number of subjects in each split for each group\n",
    "        num_per_split_0 = len(group_0) // num_of_splits\n",
    "        num_per_split_1 = len(group_1) // num_of_splits\n",
    "        \n",
    "        # Initialize splits for each group\n",
    "        splits_0 = [group_0[i * num_per_split_0:(i + 1) * num_per_split_0] for i in range(num_of_splits)]\n",
    "        splits_1 = [group_1[i * num_per_split_1:(i + 1) * num_per_split_1] for i in range(num_of_splits)]\n",
    "        \n",
    "        # Adjust for any remaining subjects\n",
    "        remainder_0 = group_0[num_per_split_0 * num_of_splits:]\n",
    "        remainder_1 = group_1[num_per_split_1 * num_of_splits:]\n",
    "        \n",
    "        for i, subj in enumerate(remainder_0):\n",
    "            splits_0[i % num_of_splits].append(subj)\n",
    "        \n",
    "        for i, subj in enumerate(remainder_1):\n",
    "            splits_1[i % num_of_splits].append(subj)\n",
    "        \n",
    "        # Combine the splits from both groups\n",
    "        splits = [split_0 + split_1 for split_0, split_1 in zip(splits_0, splits_1)]\n",
    "        \n",
    "        # Create a dictionary to map each subject to their assigned split\n",
    "        subject_to_split = {subj: i + 1 for i, split in enumerate(splits) for subj in split}\n",
    "        \n",
    "        # Assign splits to each participant and create the 'test'/'train' arrays\n",
    "        subject_indices = np.array([subject_to_split[subj] for subj in self.subj]) - 1\n",
    "        \n",
    "        split_matrix = np.ones((len(self.subj), num_of_splits))\n",
    "        split_matrix[np.arange(len(self.subj)), subject_indices] = 0\n",
    "        \n",
    "        self.splits = split_matrix\n",
    "\n",
    "    def print_all_info(self):\n",
    "        for attr_name, attr_value in self.__dict__.items():\n",
    "            print(f'{attr_name}: {attr_value}')\n",
    "    \n",
    "    def compute_average_erps(self):\n",
    "        unique_subjects = np.unique(self.subj)\n",
    "        unique_triggers = np.unique(self.triggers)\n",
    "        \n",
    "        for subj in unique_subjects:\n",
    "            subj_erps = []\n",
    "            for trigger in unique_triggers:\n",
    "                subj_trigger_epochs = self.data[:, :, (self.subject_id == subj) & (self.triggers == trigger)]\n",
    "                avg_erp = np.mean(subj_trigger_epochs, axis=2)\n",
    "                subj_erps.append(avg_erp)\n",
    "            self.average_erps[subj] = subj_erps\n",
    "\n",
    "    def get_average_erp(self, subj, trigger):\n",
    "        subj_erps = self.average_erps.get(subj, None)\n",
    "        if subj_erps is not None:\n",
    "            return subj_erps[trigger]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def cluster_test(self, channel, trigger_set1, trigger_set2):\n",
    "            mask1 = np.isin(self.triggers, trigger_set1)\n",
    "            print(mask1)\n",
    "            mask2 = np.isin(self.triggers, trigger_set2)\n",
    "            print(mask2)\n",
    "\n",
    "            data1 = self.data[mask1, :, :]\n",
    "            data2 = self.data[mask2, :, :]\n",
    "\n",
    "            #data1 = np.expand_dims(data1,0)\n",
    "            #data2 = np.expand_dims(data2,0)\n",
    "\n",
    "            print(data1.shape)\n",
    "\n",
    "            data = [data1.transpose(2, 0, 1), data2.transpose(2, 0, 1)]\n",
    "\n",
    "            # Run the cluster-based permutation test\n",
    "            T_obs, clusters, cluster_p_values, H0 = permutation_cluster_test(\n",
    "                data, n_permutations=1000, tail=0, n_jobs=-1, threshold=None, \n",
    "                out_type='mask'\n",
    "            )\n",
    "\n",
    "                # Store cluster information\n",
    "            self.T_obs = T_obs\n",
    "            self.clusters = clusters\n",
    "            self.cluster_p_values = cluster_p_values\n",
    "            self.H0 = H0\n",
    "\n",
    "            # Plot the results\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            times = np.arange(data1.shape[0])\n",
    "            for i_c, c in enumerate(clusters):\n",
    "                c = c[0]\n",
    "                if cluster_p_values[i_c] <= 0.05:\n",
    "                    h = plt.axvspan(times[c.start], times[c.stop - 1], color='r', alpha=0.3)\n",
    "            plt.plot(times, data1.mean(axis=1) - data2.mean(axis=1), label='Difference')\n",
    "            plt.legend()\n",
    "            plt.title(f'Cluster-based permutation test results for channel {channel}')\n",
    "            plt.xlabel('Time (samples)')\n",
    "            plt.ylabel('ERP Difference')\n",
    "            plt.show()\n",
    "    \n",
    "    #def subtract_noise_from_data(sub, subj_data):\n",
    "    #    print(f'Subtracting Noise Mean from {sub}')\n",
    "\n",
    "\n",
    "def create_channel_mask(total_channels, excluded_channels):\n",
    "    \"\"\"\n",
    "    Create a boolean mask to exclude specific channels from the data.\n",
    "\n",
    "    Parameters:\n",
    "    - total_channels (int): Total number of channels in the data.\n",
    "    - excluded_channels (list): List of channels to exclude from the data.\n",
    "\n",
    "    Returns:\n",
    "    - channel_mask (ndarray): Boolean mask with 1s for included channels and 0s for excluded channels.\n",
    "    \"\"\"\n",
    "    # Initialize a boolean mask with all True (included channels)\n",
    "    channel_mask = np.ones(total_channels, dtype=bool)\n",
    "    \n",
    "    # Set the indices corresponding to excluded channels to False\n",
    "    channel_mask[excluded_channels] = False\n",
    "    \n",
    "    return channel_mask\n",
    "\n",
    "def shuffle_data(dataset,labels):\n",
    "    # Create an index array to shuffle the data and labels in the same order\n",
    "    index = np.arange(len(dataset))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    # Shuffle the dataset and labels using the index array\n",
    "    shuffled_dataset = dataset[index]\n",
    "    shuffled_labels = labels[index]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "    # Now, shuffled_dataset and shuffled_labels contain the data and labels shuffled in the same order\n",
    "\n",
    "def reshape_data_ml(data):\n",
    "\n",
    "    #data = data[~np.all(data == 0, axis=1)]\n",
    "    if len(data.shape) == 4:\n",
    "\n",
    "        data = data.reshape(data.shape[0],data.shape[1]*data.shape[2],data.shape[3])\n",
    "        \n",
    "        data = data[:,~np.all(data == 0, axis=0)]\n",
    "    elif len(data.shape) == 3:\n",
    "        data = data.reshape(data.shape[0],data.shape[1]*data.shape[2])\n",
    "\n",
    "    #data = data.reshape(data.shape[0],data.shape[1]*data.shape[2])\n",
    "\n",
    "    #print(data.shape)\n",
    "\n",
    "    return data\n",
    "\n",
    "def change_labels_ml(labels):\n",
    "    lbl_clf = preprocessing.LabelEncoder() #function \n",
    "    labels = lbl_clf.fit_transform(labels)\n",
    "    #print(labels.shape)\n",
    "    #labels = tf.keras.utils.to_categorical(labels) # transform the data into columns and such\n",
    "    return labels\n",
    "\n",
    "def change_labels_cnn(labels):\n",
    "    lbl_clf = preprocessing.LabelEncoder() #function \n",
    "    labels = lbl_clf.fit_transform(labels)\n",
    "    labels = tf.keras.utils.to_categorical(labels) # transform the data into columns and such\n",
    "    print(labels.shape)\n",
    "    return labels\n",
    "\n",
    "def mask_data2(final_mask,test_mask,label_source,data):\n",
    "\n",
    "    train_data = data[final_mask,:]\n",
    "    train_labels = label_source[final_mask]\n",
    "\n",
    "    test_data = data[test_mask,:]\n",
    "    test_labels = label_source[test_mask]\n",
    "\n",
    "    test_data,test_labels = shuffle_data(test_data,test_labels)\n",
    "\n",
    "    train_data,train_labels = shuffle_data(train_data,train_labels)\n",
    "\n",
    "    final_data = [(train_data),(train_labels),(test_data),(test_labels)]\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def reshape_all(final_data):\n",
    "\n",
    "    X_train = reshape_data_ml(final_data[0])\n",
    "    X_test = reshape_data_ml(final_data[2])\n",
    "    y_train = change_labels_ml(final_data[1])\n",
    "    y_test = change_labels_ml(final_data[3])\n",
    "\n",
    "    data = [X_train,X_test,y_train,y_test]\n",
    "\n",
    "    return data\n",
    "\n",
    "def plot_avg_ERP(data):\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Assuming 'data' is your 3D array [epoch, channel, time-points]\n",
    "\n",
    "    # Calculate the average across epochs to get ERPs\n",
    "    erp_data = np.mean(data, axis=0)\n",
    "\n",
    "    # Get the time points for x-axis\n",
    "    time_points = np.arange(data.shape[2])  # Assuming time is the third dimension\n",
    "\n",
    "    # Plot ERPs for all channels\n",
    "    num_channels = data.shape[1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        plt.plot(time_points, erp_data[channel, :], label=f'Channel {channel + 1}')\n",
    "\n",
    "    plt.xlabel('Time Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Event-Related Potentials (ERPs) for All Channels')\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_model(dataset,model,title,combination):\n",
    "\n",
    "    print(f\"Running model {title}, {combination}\")\n",
    "    #print(model)\n",
    "    model.fit(dataset[0],dataset[2])\n",
    "    prediction = model.predict(dataset[1])\n",
    "    print(f\"The accuracy of the {title} , {combination} is {metrics.accuracy_score(prediction,dataset[3])}\")\n",
    "    print(f\"The Area Under the Curve of the {title} , {combination} is {metrics.roc_auc_score(prediction,dataset[3])}\")\n",
    "\n",
    "    return model,prediction\n",
    "\n",
    "def normalize_data(data):\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # Assuming 'data' is your input data\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "def clean_nones(mask):\n",
    "    #mask = np.array(mask)\n",
    "    mask2 = mask != None\n",
    "    mask3 = mask[mask2] \n",
    "    #mask4 = mask3.reshape(mask.shape[0],-1)\n",
    "    return mask3\n",
    "\n",
    "def clean_nones_all_data(data):\n",
    "    #mask = np.array(mask)\n",
    "    # Create a boolean mask indicating rows with all zeros\n",
    "    mask = np.any(data != 0, axis=2)\n",
    "\n",
    "    # Select only the rows that do not contain all zeros\n",
    "    filtered_array = data[mask]\n",
    "    return filtered_array\n",
    "\n",
    "def clean_nones_all_data_and_masks(data,masks):\n",
    "    #mask = np.array(mask)\n",
    "    # Create a boolean mask indicating rows with all zeros\n",
    "    new_mask = np.any(data != None, axis=2)\n",
    "    print(new_mask.all())\n",
    "    filtered_data = data[new_mask]\n",
    "    filtered_masks = []\n",
    "    for mask in masks:\n",
    "        # Select only the rows that do not contain all zeros\n",
    "        filtered_masks.append(mask[new_mask])\n",
    "    return filtered_masks\n",
    "\n",
    "# # Importing Lucas/Dimitri function\n",
    "# from timeit import default_timer as timer \n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Flatten, Dropout, Conv1D, Conv2D, Conv3D, MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('C:/GitHub/ML_models/Whether_Project/')\n",
    "# from custumBoost_matrix import CustomFeatureFabrique  \n",
    "\n",
    "# def Haar_wavelet2(all_data):\n",
    "#     import sys\n",
    "#     sys.path.append('C:/GitHub/ML_models/Whether_Project/')\n",
    "\n",
    "#     from custumBoost_matrix import CustomFeatureFabrique\n",
    "\n",
    "#     feature_factory = CustomFeatureFabrique(all_data, sampling_freq=500)\n",
    "#     feature_factory.generate_wavelet_parameters()\n",
    "\n",
    "#     feature_factory.generate_monophasic_wavelets(is_to_plot = False)\n",
    "#     feature_factory.generate_biphasic_wavelets(is_to_plot = False)\n",
    "    \n",
    "#     bi_features = feature_factory.generate_biphasic_convolution_features()\n",
    "#     mono_features = feature_factory.generate_monophasic_convolution_features()\n",
    "#     #reg_features = feature_factory.generate_regression_features()\n",
    "    \n",
    "#     bi_features = np.array(bi_features)\n",
    "#     bi_features = np.transpose(bi_features, (1, 2, 0))\n",
    "#     #print(bi_features.shape)\n",
    "#     mono_features = np.array(mono_features)\n",
    "#     mono_features = np.transpose(mono_features, (1, 2, 0))\n",
    "#     #print(mono_features.shape)\n",
    "\n",
    "#     #reg_features = np.array(reg_features)\n",
    "#     #print(reg_features.shape)\n",
    "\n",
    "#     #reg_features = np.transpose(reg_features, (2, 1, 0))\n",
    "#     #print(reg_features.shape)\n",
    "    \n",
    "#     all_features = np.append(mono_features,bi_features,axis=2)\n",
    "#     #all_features = np.append(all_features,reg_features,axis=2)\n",
    "\n",
    "#     return all_features,mono_features,bi_features\n",
    "    \n",
    "# #all_features, mono_features,bi_features = Haar_wavelet(all_data)\n",
    "\n",
    "\n",
    "def average_trials_nosubs(data, labels, avg_num):\n",
    "    averaged_data = []\n",
    "    averaged_labels = []\n",
    "    print(data.shape)\n",
    "\n",
    "    # Separate data based on labels\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        #print(\"RUNNING\")\n",
    "        label_data = data[labels == label]\n",
    "        num_trials = label_data.shape[0]\n",
    "        num_batches = num_trials // avg_num\n",
    "\n",
    "        # Loop over the data averaging every 8 trials\n",
    "        for i in range(num_batches):\n",
    "            start_index = i * avg_num\n",
    "            end_index = start_index + avg_num\n",
    "            batch_data = label_data[start_index:end_index]\n",
    "            averaged_trial = np.mean(batch_data, axis=0)\n",
    "            averaged_data.append(averaged_trial)\n",
    "            averaged_labels.append(label)\n",
    "\n",
    "    return np.array(averaged_data), np.array(averaged_labels)\n",
    "\n",
    "def average_trials(data, labels, average_trials=5):\n",
    "\n",
    "    if average_trials < 2:\n",
    "        averaged_data = data\n",
    "        averaged_labels = labels\n",
    "    else:\n",
    "\n",
    "        averaged_data = []\n",
    "        averaged_labels = []\n",
    "\n",
    "        # Separate data based on labels\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            label_data = data[labels == label]\n",
    "            num_trials = label_data.shape[0]\n",
    "            \n",
    "            # Calculate number of batches needed to sample with substitution\n",
    "            num_batches = int(np.ceil(len(label_data) / average_trials))\n",
    "\n",
    "            # Loop over the data and collect averages with substitution\n",
    "            for _ in range(num_trials):\n",
    "                # Sample with replacement\n",
    "                indices = np.random.choice(len(label_data), average_trials, replace=True)\n",
    "                batch_data = label_data[indices]\n",
    "                \n",
    "                # Compute average and append to list\n",
    "                averaged_trial = np.mean(batch_data, axis=0)\n",
    "                averaged_data.append(averaged_trial)\n",
    "                averaged_labels.append(label)\n",
    "\n",
    "    return np.array(averaged_data), np.array(averaged_labels)\n",
    "\n",
    "\n",
    "def balance_trials(data, labels, average_trials=5):\n",
    "    if average_trials < 2:\n",
    "        return data, labels\n",
    "\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    max_trials = counts.max()\n",
    "    \n",
    "    #print(max_trials)\n",
    "\n",
    "    balanced_data = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_data = data[labels == label]\n",
    "        num_trials = label_data.shape[0]\n",
    "        \n",
    "        # else:\n",
    "        for _ in range(max_trials):\n",
    "            indices = np.random.choice(len(label_data), average_trials, replace=True)\n",
    "            batch_data = label_data[indices]\n",
    "            averaged_trial = np.mean(batch_data, axis=0)\n",
    "            balanced_data.append(averaged_trial)\n",
    "            balanced_labels.append(label)\n",
    "            \n",
    "    #print(len(balanced_labels))\n",
    "    return np.array(balanced_data), np.array(balanced_labels)\n",
    "\n",
    "def run_cnn(X_train,y_train,X_test, y_test):\n",
    "    \n",
    "\n",
    "    std_clf = StandardScaler() # standardize the data\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    lbl_clf = preprocessing.LabelEncoder() #function \n",
    "    print(y_train)\n",
    "    print(y_test)\n",
    "    y_train = lbl_clf.fit_transform(y_train)\n",
    "    y_test = lbl_clf.fit_transform(y_test)\n",
    "    # encode output to classes\n",
    "    print(y_train)\n",
    "    print(y_test)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train) # transform the data into columns and such\n",
    "    y_test = tf.keras.utils.to_categorical(y_test) # transform the data into columns and such\n",
    "\n",
    "    print(X_train.shape)\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "    tf.keras.backend.clear_session() # if we dont do that, if you would rerun this, it would take the previous model\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    # Create the optimizer with the custom learning rate\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "\n",
    "    # For train test between different experiments\n",
    "\n",
    "    cnn = tf.keras.models.Sequential() # sequential mode 99% of the time;\n",
    "\n",
    "    cnn.add(Flatten(input_shape=(X_train.shape[0:])))\n",
    "    #model.add(Conv2D(32,(2,2),activation='relu',input_shape=(X_train.shape[1:])))\n",
    "    cnn.add(Dense(100, activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    cnn.add(Dense(100, activation='relu'))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(tf.keras.layers.Dense(y_test.shape[1], activation='relu', name='output'))\n",
    "    # Set the learning rate\n",
    "    \n",
    "    loss = 'categorical_crossentropy',  # Choose a loss function\n",
    "    metrics = 'accuracy',  # Choose evaluation metrics\n",
    "    num_epochs = 10,  # Set the default number of epochs\n",
    "    batch_size = 10,  # Set the default batch size\n",
    "    verbose = 1,  # Set the default verbosity level (0 for silent, 1 for progress bar, 2 for one line per epoch)\n",
    "    validation_split = 0.25  # Set the default validation split\n",
    "\n",
    "    cnn.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    print(cnn.summary())\n",
    "\n",
    "    history = cnn.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=verbose,validation_split=validation_split)\n",
    "    #cnn.fit(X_train, y_train, nb_epoch=20, batch_size=20, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "    cnn.evaluate(X_test, y_test)\n",
    "\n",
    "    return cnn\n",
    "\n",
    "\n",
    "class This_Run:\n",
    "    def __init__(self, subject, round_number, split_number, y_variable, step, window, average, model, percent, mods):\n",
    "        # Data, Labels, Subject\n",
    "        self.subject = subject\n",
    "        #self.subject.data = [] # CLEANS THE DATASET OUT OF THE RUN FOR FILESIZE\n",
    "        self.y_variable = y_variable\n",
    "        self.split_number = split_number\n",
    "        self.description = mods\n",
    "        self.round_number = round_number\n",
    "        # Variables and Hyperparameters\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.average = average\n",
    "        self.percent = percent\n",
    "        self.model = model\n",
    "        \n",
    "        #self.channels = channels_mask\n",
    "\n",
    "    def create_masks(self):\n",
    "        \n",
    "        masks = {\n",
    "            'IB': (self.subject.experiment == 'IB'),\n",
    "            'DC': (self.subject.experiment == 'DC'),\n",
    "            'BM': (self.subject.experiment == 'BM'),\n",
    "            'house': (self.subject.stimulus == 'house'),\n",
    "            'face': (self.subject.stimulus == 'face'),\n",
    "            'noise': (self.subject.stimulus == 'noise'),\n",
    "            'visible': (self.subject.phase == 'visible'),\n",
    "            'invisible': (self.subject.phase == 'invisible'),\n",
    "            'test': (np.array(self.subject.split) == 'test'),\n",
    "            'train': (np.array(self.subject.split) == 'train'),\n",
    "            'blinded': (self.subject.groups == 1),\n",
    "            'not_blinded': (self.subject.groups == 0)\n",
    "        }\n",
    "        self.masks = masks\n",
    "        #return masks\n",
    "    \n",
    "    def mask_data_multipletests(self,train_mask, test_masks, label_source, data, round_name):\n",
    "\n",
    "        #print('Data shape',data.shape)\n",
    "        #print('Mask shape',train_mask.shape)\n",
    "        #print('test Mask shape',test_masks.shape)\n",
    "        train_data = data[train_mask, ...]\n",
    "        train_labels = label_source[train_mask]\n",
    "        \n",
    "        all_test_data = []\n",
    "        all_tests_labels = []\n",
    "        \n",
    "        for test_mask in test_masks:\n",
    "            test_data = data[test_mask, ...]\n",
    "            test_labels = label_source[test_mask]\n",
    "            test_data, test_labels = shuffle_data(test_data, test_labels)\n",
    "            all_test_data.append(test_data)\n",
    "            all_tests_labels.append(test_labels)\n",
    "            #print(test_labels)\n",
    "\n",
    "        train_data, train_labels = shuffle_data(train_data, train_labels)\n",
    "\n",
    "        #print('Train data shape',train_data.shape, 'Train labels shape',train_labels.shape)\n",
    "\n",
    "        return train_data, train_labels, all_test_data, all_tests_labels, round_name\n",
    "    \n",
    "    def create_masks_from_choices(self,mask_choices):\n",
    "        combined_mask = np.ones_like(self.subject.phase, dtype=bool)\n",
    "\n",
    "        for choice in mask_choices:\n",
    "            temp_mask = np.zeros_like(self.subject.phase, dtype=bool)\n",
    "            #print(temp_mask.shape)\n",
    "            for mask in choice:\n",
    "                #print('Mask:', mask)\n",
    "                #print('Mask Shape', self.masks[mask].shape)\n",
    "                if 'test' in mask or 'train' in mask:\n",
    "                    temp_mask |= self.masks[mask][:, self.split_number]\n",
    "                else:\n",
    "                    temp_mask |= self.masks[mask]\n",
    "            combined_mask &= temp_mask\n",
    "        #print('Combined mask', combined_mask.shape)\n",
    "        return combined_mask\n",
    "\n",
    "\n",
    "    def apply_masks(self):\n",
    "        \n",
    "        masks = self.masks\n",
    "        final_data = []\n",
    "        \n",
    "        #for i in range(subject.num_splits):\n",
    "        #print(f'split number {i}')\n",
    "        final_masks = []\n",
    "        final_test_masks = []\n",
    "        round_robin_labels = []\n",
    "        \n",
    "        # Create masks for the current split\n",
    "\n",
    "        #print(masks['test'])\n",
    "\n",
    "        final_masks = self.create_masks_from_choices(self.subject.train_masks[self.round_number])\n",
    "        final_test_masks = [self.create_masks_from_choices(test) for test in self.subject.test_masks[self.round_number]]\n",
    "\n",
    "        # final_masks.append((masks['face'] + masks['house']) * masks['invisible'] * masks['train'][:, self.split_number])\n",
    "        # final_test_masks.append((masks['face'] + masks['house']) * masks['invisible'] * masks['test'][:, self.split_number])\n",
    "        # final_test_masks.append(masks['face'] * masks['invisible'] * masks['test'][:, self.split_number])\n",
    "        # final_test_masks.append(masks['house'] * masks['invisible'] * masks['test'][:, self.split_number])\n",
    "        \n",
    "        #round_robin_labels.append('Train ALL Test ALL')\n",
    "        #round_robin_labels.append('Train ALL Test Face')\n",
    "        #round_robin_labels.append('Train ALL Test House')\n",
    "        \n",
    "        #final_test_masks_groups = [final_test_masks]\n",
    "        #round_robin_group = [round_robin_labels]\n",
    "        \n",
    "        split_data = []\n",
    "        \n",
    "        #for j in range(len(final_masks)):\n",
    "        #print(f'Masking Data')\n",
    "        final_masks_arr = np.array(final_masks)\n",
    "        #print(final_masks_arr.shape)\n",
    "        final_test_masks_groups_arr = np.array(final_test_masks)\n",
    "        #print('test mask grops',final_test_masks_groups_arr.shape)\n",
    "\n",
    "        # Mask the data, and collect data and labels for test and train\n",
    "        split_data = (\n",
    "            self.mask_data_multipletests(\n",
    "                final_masks_arr,\n",
    "                final_test_masks_groups_arr,\n",
    "                self.y_variable,\n",
    "                self.subject.data,\n",
    "                self.subject.round_robin_labels\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #final_data.append(split_data)\n",
    "        print('Train data',split_data[0].shape)\n",
    "        \n",
    "        # store variables\n",
    "        self.dataset = split_data\n",
    "        self.pipeline_labels = self.subject.round_robin_labels\n",
    "\n",
    "    def print_info(self):\n",
    "        print(f'Subject: {self.subject}')\n",
    "        print(f'Steps: {self.step}')\n",
    "        print(f'Windows: {self.window}')\n",
    "        print(f'Averages: {self.average}')\n",
    "        print(f'Percent: {self.percent}')\n",
    "        print(f'Models: {self.models}')\n",
    "        print(f'Mods: {self.mods}')\n",
    "\n",
    "       \n",
    "    def remove_noise_average(self):\n",
    "        \"\"\"\n",
    "        Remove the average noise from data for the current split (self.split_number).\n",
    "        This function computes the noise average for trainig and testing datasets in this split and applies it them separately\n",
    "        \"\"\"\n",
    "        print('Removing Noise from Dataset')\n",
    "        # Get the current split's train and test indices\n",
    "        train_indices = [i for i, split in enumerate(self.subject.split) if split[self.split_number] == 'train']\n",
    "        test_indices = [i for i, split in enumerate(self.subject.split) if split[self.split_number] == 'test']\n",
    "        print('Train indices: ',train_indices)\n",
    "        print('Test indices: ',test_indices)\n",
    "\n",
    "        # Get the noise data for the training set only\n",
    "        train_noise_data = np.array([self.subject.data[i,:,:] for i in train_indices if self.subject.stimulus[i] == 'noise'])\n",
    "        test_noise_data = np.array([self.subject.data[i,:,:] for i in test_indices if self.subject.stimulus[i] == 'noise'])\n",
    "\n",
    "        print('Train Noise Shape: ',train_noise_data.shape)\n",
    "        \n",
    "        # Compute the average noise for this split's training data\n",
    "        train_noise_average = np.mean(train_noise_data, axis=0)\n",
    "        print('Train Noise avg Shape: ',train_noise_average.shape)\n",
    "        \n",
    "        # Collect Training data and print ERP\n",
    "        training_data = self.subject.data[train_indices]\n",
    "        title = 'Pre Removal Training Data ERP subject {self.subject.subject} Round {self.round_number} Split {self.split_number}'\n",
    "        #print_ERP(np.mean(training_data,axis=0),title)\n",
    "        \n",
    "        # Remove noise avg. and print ERP\n",
    "        print('training data shape: ', self.subject.data[train_indices].shape)\n",
    "        self.subject.data[train_indices] -= train_noise_average\n",
    "        title = 'Post Removal Training Data ERP subject {self.subject.subject} Round {self.round_number} Split {self.split_number}'\n",
    "        #print_ERP(np.mean(self.subject.data[train_indices],axis=0),title)\n",
    "\n",
    "\n",
    "        # Compute the average noise for this split's testing data\n",
    "        test_noise_average = np.mean(test_noise_data, axis=0)\n",
    "        # Collect Testing data and print ERP\n",
    "        \n",
    "        testing_data = self.subject.data[test_indices]\n",
    "        title = 'Pre Removal Testing Data ERP subject {self.subject.subject} Round {self.round_number} Split {self.split_number}'\n",
    "        #print_ERP(np.mean(testing_data,axis=0),title)\n",
    "\n",
    "        # Remove noise avg. and print ERP\n",
    "        print('training data shape: ', self.subject.data[train_indices].shape)\n",
    "        self.subject.data[test_indices] -= test_noise_average\n",
    "        title = 'Pre Removal Testing Data ERP subject {self.subject.subject} Round {self.round_number} Split {self.split_number}'\n",
    "        #print_ERP(np.mean(self.subject.data[test_indices],axis=0),title)\n",
    "                \n",
    "        print('Subtraction Completed')\n",
    "            \n",
    "\n",
    "    def train_and_test_window(self, start_time, train_data_slice, y_train, all_test_data_slices, all_test_labels):\n",
    "        try:\n",
    "            # Debugging: Log process info\n",
    "            print(f\"Process {os.getpid()} working on start_time: {start_time}\")\n",
    "\n",
    "            end_time = start_time + self.window\n",
    "\n",
    "            train_data_slice = np.array(train_data_slice)\n",
    "            #print('Train Slice Shape',train_data_slice.shape)\n",
    "            #print('length of all test slices',len(all_test_data_slices))\n",
    "            \n",
    "            X_train = train_data_slice.reshape(train_data_slice.shape[0], -1) if len(train_data_slice.shape) == 3 else train_data_slice\n",
    "            X_train = normalize_data(X_train)\n",
    "            y_train = change_labels_ml(y_train)\n",
    "            #print('Y train: ',y_train)\n",
    "            \n",
    "            classifier = copy.deepcopy(self.model)\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            results = []\n",
    "            score = []\n",
    "            f1score = []\n",
    "            auc = []\n",
    "            auc2 = []\n",
    "            ci = []\n",
    "\n",
    "            for i, test_data_slice in enumerate(all_test_data_slices):\n",
    "                labels_test = all_test_labels[i]\n",
    "                test_data_slice = np.array(test_data_slice)\n",
    "                #print('Test Slice Shape',test_data_slice.shape)\n",
    "\n",
    "                for selected_test_slice in test_data_slice:\n",
    "                #for start_time2 in range(0, test_data_slice.shape[2] - self.window - 1, self.step):\n",
    "                    #end_time2 = start_time2 + self.window\n",
    "                    #selected_test_slice = test_data_slice[:, :, start_time2:end_time2]\n",
    "                    #print('selected test slice shape: ',selected_test_slice.shape)\n",
    "\n",
    "                    data_test = selected_test_slice.reshape(selected_test_slice.shape[0], -1)\n",
    "                    data_test = normalize_data(data_test)\n",
    "                    #print('selected test slice shape: ',data_test.shape)\n",
    "\n",
    "                    y_pred = classifier.predict(data_test)\n",
    "                    y_test = labels_test\n",
    "                    #print('y_test: ',y_test)\n",
    "                    y_test = change_labels_ml(y_test)\n",
    "\n",
    "                    score.append(metrics.accuracy_score(y_pred, y_test))\n",
    "                    f1score.append(metrics.f1_score(y_pred, y_test, average='weighted'))\n",
    "                    auc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "                    roc_results = roc_auc_score(y_test, y_pred, confidence_level=0.95)\n",
    "                    auc2.append(roc_results[0])\n",
    "                    ci.append(roc_results[1])\n",
    "\n",
    "            return score, f1score, auc, auc2, ci\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in window {start_time}-{end_time}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return [], [], [], [], []\n",
    "\n",
    "\n",
    "    def moving_window_subsample_par(self):\n",
    "        try:\n",
    "            print(\"Starting moving_window_subsample...\")\n",
    "\n",
    "            self.remove_noise_average()\n",
    "            self.create_masks()\n",
    "            self.apply_masks()\n",
    "\n",
    "            scores, f1scores, aucs, auc2s, cis = [], [], [], [], []\n",
    "\n",
    "            train_data, train_labels = shuffle_data(self.dataset[0], self.dataset[1])\n",
    "            #train_data, train_labels = balance_trials(train_data, train_labels, self.average)\n",
    "            #print(train_labels.shape)\n",
    "            \n",
    "            all_test_data = self.dataset[2]\n",
    "            all_test_labels = self.dataset[3]\n",
    "\n",
    "            # Clear dataset to save memory\n",
    "            self.dataset = []\n",
    "\n",
    "            train_data_slices = [train_data[:, :, start_time:start_time + self.window] \n",
    "                                for start_time in range(0, train_data.shape[2] - self.window - 1, self.step)]\n",
    "            train_data_slices = np.array(train_data_slices)\n",
    "\n",
    "            all_test_data_slices = [\n",
    "                [test_data[:, :, start_time:start_time + self.window] \n",
    "                for start_time in range(0, test_data.shape[2] - self.window - 1, self.step)]\n",
    "                for test_data in all_test_data\n",
    "            ]\n",
    "\n",
    "            print(f\"Submitting {len(train_data_slices)} tasks to the executor...\")\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:  # Limit to 4 workers (or adjust as necessary)\n",
    "                futures = [\n",
    "                    executor.submit(self.train_and_test_window, start_time, train_data_slices[i], train_labels, all_test_data_slices, all_test_labels)\n",
    "                    for i, start_time in enumerate(range(0, train_data.shape[2] - self.window - 1, self.step))\n",
    "                ]\n",
    "            \n",
    "            # chunk_size = 10  # Process 10 tasks at a time\n",
    "            # chunks = [train_data_slices[i:i + chunk_size] for i in range(0, len(train_data_slices), chunk_size)]\n",
    "\n",
    "            # for chunk in chunks:\n",
    "            #     #with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor: # Should run in the server\n",
    "            #     with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor: # Runs in vscode but not in parallel\n",
    "            #         futures = [\n",
    "            #             executor.submit(self.train_and_test_window, start_time, slice, train_labels, all_test_data_slices, all_test_labels)\n",
    "            #             for start_time, slice in zip(range(0, train_data.shape[2] - self.window - 1, self.step), chunk)\n",
    "            #         ]\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    score, f1score, auc, auc2, ci = future.result()\n",
    "                    scores.append(score)\n",
    "                    f1scores.append(f1score)\n",
    "                    aucs.append(auc)\n",
    "                    auc2s.append(auc2)\n",
    "                    cis.append(ci)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process future: {e}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "            self.scores = scores\n",
    "            self.f1scores = f1scores\n",
    "            self.aucs = aucs\n",
    "            self.auc2s = auc2s\n",
    "            self.cis = cis\n",
    "\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in moving_window_subsample: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "    def moving_window_subsample(self):\n",
    "        \n",
    "        self.remove_noise_average()\n",
    "        # train_indices = [i for i, split in enumerate(self.subject.split) if split[self.split_number] == 'train']\n",
    "        # title = 'Pre removal Training Data ERP subject {self.subject.subject} Round {self.round_number} Split {self.split_number}'\n",
    "        # print_ERP(np.mean(self.subject.data[train_indices],axis=0),title)\n",
    "        \n",
    "        # test_indices = [i for i, split in enumerate(self.subject.split) if split[self.split_number] == 'test']\n",
    "        # title = 'Post removal Testing Data ERP subject {self.subject.subject} Round {self.round_number} Split {self.split_number}'\n",
    "        # print_ERP(np.mean(self.subject.data[test_indices],axis=0),title)\n",
    "\n",
    "\n",
    "        self.create_masks()\n",
    "        self.apply_masks()\n",
    "        \n",
    "        #self.subject.data = []\n",
    "\n",
    "        print('Starting Moving Window Subsample')\n",
    "        # create empty variables\n",
    "        scores = []\n",
    "        models = []\n",
    "        predictions = []\n",
    "        aucs = []\n",
    "        f1scores = []\n",
    "        auc2s = []\n",
    "        cis = []\n",
    "\n",
    "        # collect data from the dataset variable\n",
    "        train_data = self.dataset[0]\n",
    "        print(train_data.shape)\n",
    "        train_labels = self.dataset[1]\n",
    "        print(train_labels.shape)\n",
    "        all_test_data = self.dataset[2]\n",
    "        #print(all_test_data.shape)\n",
    "        all_test_labels = self.dataset[3]\n",
    "        #print(all_test_data.shape)\n",
    "        self.dataset = [] # CLEANS THE DATASET OUT OF THE RUN FOR FILESIZE\n",
    "        \n",
    "        #print('Shuffling data')\n",
    "        train_data, train_labels = shuffle_data(train_data, train_labels)\n",
    "        #print('Averaging data')\n",
    "        train_data, train_labels = balance_trials(train_data, train_labels, self.average)\n",
    "\n",
    "        # SELECT WINDOW OF TIME\n",
    "        #print('Filtering Data')\n",
    "        #train_data = train_data[:,:,:]\n",
    "\n",
    "        #print(train_data.shape)\n",
    "\n",
    "        # THIS RANDOMIZES THE DATA AND ALLOWS FOR SUB-SELECTION OF DATA FOR SMALLER TEST TRAININGS\n",
    "        X_tr = train_data.shape[0]\n",
    "        num_ones_tr = int(X_tr * (self.percent / 100)) # Calculate the number of 1s based on the percentage\n",
    "        random_array_train = np.random.choice([False, True], size=X_tr, p=[(100 - self.percent) / 100, self.percent / 100]) # Generate the array with the specified percentage of 1s\n",
    "        \n",
    "        # Starts the loop for timewindows of the test data.\n",
    "        self.array_of_windows = enumerate(range(0, (train_data.shape[2] - self.window -1), self.step))\n",
    "        #print(self.array_of_windows)\n",
    "\n",
    "        results = []\n",
    "        score = []\n",
    "        f1score = []\n",
    "        auc = []\n",
    "        auc2 = []\n",
    "        ci = []\n",
    "        y_tests = []\n",
    "        print('Starting Training Loop')\n",
    "        for k, start_time in enumerate(range(0, (train_data.shape[2] - self.window -1), self.step)):\n",
    "            \n",
    "            start = timer()\n",
    "            end_time = start_time + self.window\n",
    "            \n",
    "            ### TRAIN ###\n",
    "\n",
    "            # Use random array to select train data to be run\n",
    "            train_data_win = train_data[random_array_train,:,start_time:end_time]\n",
    "            y_train = train_labels[random_array_train]  \n",
    "\n",
    "            # perform Haar transformation if necessary\n",
    "            #if haar:\n",
    "            #    train_data_win,mono,bi = Haar_wavelet2(train_data_win)\n",
    "\n",
    "            # test for shape, make sure it's in 2D for MLs\n",
    "            X_test = []\n",
    "            X_train = []\n",
    "\n",
    "            if len(train_data_win.shape) == 3:\n",
    "                X_train = train_data_win.reshape(train_data_win.shape[0],train_data_win.shape[1]*train_data_win.shape[2])\n",
    "            else:\n",
    "                X_train = train_data_win\n",
    "            \n",
    "            #print('X Train Data', X_train.shape)\n",
    "            X_train = normalize_data(X_train)\n",
    "            y_train = change_labels_ml(y_train)\n",
    "\n",
    "            ### RUN THE MODEL ###\n",
    "            classifier = self.model\n",
    "            classifier.fit(X_train,y_train)\n",
    "            #print_mean_coeff(classifier.coef_,0)\n",
    "            #try\n",
    "            models.append(copy.deepcopy(classifier))\n",
    "\n",
    "            ### TEST ###\n",
    "\n",
    "            # initializing variables for test_data\n",
    "            comparing_test_data_win = []\n",
    "            test_data_each_win = []\n",
    "            comparing_label_data_win = []\n",
    "            y_test_each_win = [] \n",
    "\n",
    "            # initialize variables to store scores\n",
    "            prediction = []\n",
    "            results = []\n",
    "            score = []\n",
    "            f1score = []\n",
    "            auc = []\n",
    "            auc2 = []\n",
    "            ci = []\n",
    "            y_test = []\n",
    "            random_array_test = []\n",
    "\n",
    "            start_time2_array = []\n",
    "\n",
    "            # DO ALL TEST DATA THINGS FIRST\n",
    "            # THEN LOOP THROUGH ALL TEST DATA Possibilities\n",
    "            for i, test in enumerate(all_test_data):\n",
    "                #print(f'Starting Data Testing{i}')\n",
    "                #print(test)\n",
    "                labels_test = all_test_labels[i]\n",
    "                test, labels_test = shuffle_data(test, labels_test)\n",
    "                test, labels_test = balance_trials(test, labels_test, self.average)\n",
    "\n",
    "                test = test[:,:,:]\n",
    "                y_test = []\n",
    "                X_test = []\n",
    "                title_test = []\n",
    "                # DO LOOPS THROUGH ALL THE POSSIBLE TIME-WINDOWS\n",
    "                for j, start_time2 in enumerate(range(0, (train_data.shape[2] - self.window -1), self.step)):\n",
    "                    #print(j)\n",
    "                    #print(f'Starting Running Window {j}')\n",
    "                    end_time2 = start_time2 + self.window\n",
    "                    start_time2_array.append(start_time2)\n",
    "                    # randomly subsample the data\n",
    "                    ### DATA ###\n",
    "                    X_te = test.shape[0]\n",
    "                    num_ones_te = int(X_te * (self.percent / 100))\n",
    "                    random_array_test = np.random.choice([False, True], size=X_te, p=[(100 - self.percent) / 100, self.percent / 100])\n",
    "\n",
    "                    # select subsample and window of the data\n",
    "                    #print(test.shape)\n",
    "                    selected_test = test[random_array_test,:,start_time2:end_time2] # FILTER THE DATA FOR TIME AND SUBSAMPLE\n",
    "                    #print('loop',i)\n",
    "                    #print('time',j)\n",
    "                    #print('test data',selected_test)\n",
    "                    # perform Haar transformation if necessary\n",
    "                    #if self.haar:\n",
    "                        #train_data_win,mono,bi = Haar_wavelet2(train_data_win)\n",
    "                    #    selected_test,mono,bi = Haar_wavelet2(selected_test)\n",
    "\n",
    "                    # RESHAPE THE DATA\n",
    "                    if len(train_data_win.shape) == 3:\n",
    "                        #for test in test_data_win:\n",
    "                        data_test = selected_test.reshape(selected_test.shape[0],selected_test.shape[1]*selected_test.shape[2])\n",
    "                    else:\n",
    "                        #for test in test_data_win:\n",
    "                        data_test = selected_test\n",
    "                    data_test = normalize_data(data_test) # NORMALIZE DATA\n",
    "                    X_test.append(data_test)\n",
    "\n",
    "                    ### LABELS ###\n",
    "                    #labels_test = all_test_labels[i]\n",
    "                    #print(labels_test.shape)\n",
    "                    labels_test = change_labels_ml(labels_test) # FIX LABELS\n",
    "                    y_test.append(labels_test[random_array_test])\n",
    "                    \n",
    "                # collect all data and labels\n",
    "                comparing_test_data_win.append(X_test)\n",
    "                comparing_label_data_win.append(y_test)\n",
    "            \n",
    "            #print(np.array(comparing_label_data_win).shape)\n",
    "\n",
    "            # initialize variables\n",
    "            #time_predictions = []\n",
    "            \n",
    "            tests_predictions = []\n",
    "            tests_scores = []\n",
    "            tests_results = []\n",
    "            tests_score = []\n",
    "            tests_f1scores = []\n",
    "            tests_aucs = []\n",
    "            tests_auc2s = []\n",
    "            tests_cis = []\n",
    "            tests_y_tests = []\n",
    "            tests_models = []\n",
    "\n",
    "            # Run all tests, crespaheollect all outputs\n",
    "            for i, X_tests in enumerate(comparing_test_data_win):\n",
    "                #print(f'looping through all various tests {i}')\n",
    "                score = []\n",
    "                auc2 = []\n",
    "                auc = []\n",
    "                f1score = []\n",
    "                ci = []\n",
    "\n",
    "                time_predictions = []\n",
    "                test_ys = []\n",
    "                all_tests_predictions = []\n",
    "                #models = []\n",
    "\n",
    "                theselabels = comparing_label_data_win[i]\n",
    "                for j, X_test in enumerate(X_tests):\n",
    "                    #print(f'looping through each time {j}')\n",
    "                    #print('loop',i)\n",
    "                    #print('time',j)\n",
    "                    test_y = theselabels[j]\n",
    "                    y_pred = copy.deepcopy(classifier.predict(X_test))\n",
    "                    #print('y pred shape',y_pred.shape)\n",
    "                    time_predictions.append(y_pred)\n",
    "                    #print('test data',X_test)\n",
    "                    if hasattr(classifier, 'coef_'):\n",
    "                        model_params = {\n",
    "                            'coef_': classifier.coef_,\n",
    "                            'intercept_': classifier.intercept_,\n",
    "                            'classes_': classifier.classes_\n",
    "                        }\n",
    "                        models.append(copy.deepcopy(model_params))\n",
    "                    else:\n",
    "                        models.append(copy.deepcopy(classifier))\n",
    "                    #except:\n",
    "                    #    models.append(copy.deepcopy(classifier))\n",
    "\n",
    "                    score.append(metrics.accuracy_score(y_pred,test_y))\n",
    "                    #try:\n",
    "                    #    f1score.append(metrics.f1_score(y_pred,test_y))\n",
    "                    #except:\n",
    "                    f1score.append(0.5)\n",
    "                    try:\n",
    "                        auc.append(metrics.roc_auc_score(y_pred,test_y))\n",
    "                    except:\n",
    "                        auc.append(0.5)\n",
    "                    try:\n",
    "                        roc_results = roc_auc_score(test_y,\n",
    "                            y_pred,\n",
    "                            confidence_level=0.95)\n",
    "                    except:\n",
    "                        roc_results = ([0.5],[0.5, 0.5])\n",
    "                        #roc_results[1] = [0.5, 0.5]\n",
    "\n",
    "                    auc2.append(roc_results[0])\n",
    "                    ci.append(roc_results[1])\n",
    "\n",
    "                    significance = 'NOT sig.'\n",
    "                    #if roc_results[1][0] > 0.50:\n",
    "                        #significance = 'SIG! YES'\n",
    "                        #if j == k:\n",
    "                        #self.pipeline_labels\n",
    "                        #self.pipeline_labels[i]\n",
    "                            #print(f\"Subj. {self.subject.subject_id[0]},{self.pipeline_labels[i]} #{k} Train time {start_time}, Test time {start_time2_array[j]}, {significance} Low CI:{round(ci[j][0]*100,2)} AUC {round(auc[j]*100,2)}, accuracy {round(score[j]*100,2)},f1 score {round(f1score[j]*100,2)}, {mods}, time:\", round(timer()-start,2))\n",
    "                \n",
    "                    #print(f\"{name[i]}#{k} Train time {start_time}, Test time {start_time2_array[j]}, {significance} Low CI:{round(ci[j][0]*100,2)} AUC {round(auc[j]*100,2)}, accuracy {round(score[j]*100,2)},f1 score {round(f1score[j]*100,2)}, {mods}, time:\", round(timer()-start,2))\n",
    "                    test_ys.append(test_y)\n",
    "                    tests_predictions.append(copy.deepcopy(time_predictions))\n",
    "\n",
    "                #tests_models.append(copy.deepcopy(models))\n",
    "\n",
    "                tests_aucs.append(auc)\n",
    "                tests_auc2s.append(auc2)\n",
    "                tests_cis.append(ci)\n",
    "                tests_f1scores.append(f1score)\n",
    "                \n",
    "                all_tests_predictions.append(tests_predictions)\n",
    "                tests_y_tests.append(test_ys)\n",
    "                tests_scores.append(score)\n",
    "\n",
    "            aucs.append(tests_aucs)\n",
    "            auc2s.append(tests_auc2s)\n",
    "            cis.append(tests_cis)\n",
    "            f1scores.append(tests_f1scores)\n",
    "            all_models = tests_models\n",
    "            predictions.append(all_tests_predictions)\n",
    "            y_tests.append(tests_y_tests)\n",
    "            #print(y_tests)\n",
    "            scores.append(tests_scores)\n",
    "        \n",
    "        self.scores = scores\n",
    "        self.f1scores = f1scores\n",
    "        self.aucs = aucs\n",
    "        self.models = models\n",
    "        #self.tests_predictions = tests_predictions\n",
    "        self.auc2s = auc2s\n",
    "        self.cis = cis\n",
    "        self.y_tests = y_tests\n",
    "        self.predictions = predictions\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Example usage:\n",
    "    #pipeline = This_pipeline(subject=\"Subject 1\", self.steps=10, windows=5, averages=3, percent=0.8, models=[\"Model A\", \"Model B\"], mods={\"mod1\": 1, \"mod2\": 2})\n",
    "    #pipeline1 = This_pipeline(subj1, subject, all_data, round_robins, models, self.steps, windows, averages, percent, mods)\n",
    "\n",
    "    #pipeline1.print_info()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "#pipeline = This_pipeline(subject=\"Subject 1\", self.steps=10, windows=5, averages=3, percent=0.8, models=[\"Model A\", \"Model B\"], mods={\"mod1\": 1, \"mod2\": 2})\n",
    "#run = This_run(subj1, self.step_size, windows, avgs, percent, models, mods)\n",
    "\n",
    "#run.print_info()\n",
    "\n",
    "import datetime\n",
    "\n",
    "class This_Pipeline:\n",
    "    def __init__(self, subject, y_variable, models, models_names, steps, windows, averages, percent, mods):\n",
    "        # data\n",
    "        self.subject = copy.deepcopy(subject)\n",
    "        #self.subject.data = [] # CLEANS THE DATASET OUT OF THE RUN FOR FILESIZE\n",
    "        #self.final_data = final_data\n",
    "        self.y_variable = y_variable\n",
    "        #self.round_robins = round_robins\n",
    "        self.description = mods\n",
    "        # variables for all the pipeline runs\n",
    "        self.steps = steps\n",
    "        self.windows = windows\n",
    "        self.averages = averages\n",
    "        self.percent = percent\n",
    "        self.models = models\n",
    "        self.models_names = models_names\n",
    "        #self.channels = channels_mask\n",
    "\n",
    "        self.runs = []\n",
    "\n",
    "    def print_info(self):\n",
    "        print(f'Subject: {self.subject}')\n",
    "        print(f'Steps: {self.steps}')\n",
    "        print(f'Windows: {self.windows}')\n",
    "        print(f'Averages: {self.averages}')\n",
    "        print(f'Percent: {self.percent}')\n",
    "        print(f'Models: {self.models}')\n",
    "        print(f'Mods: {self.mods}')\n",
    "\n",
    "    def run_models(self):\n",
    "\n",
    "        self.model_outputs = []\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        try:\n",
    "\n",
    "            for i, model in enumerate(self.models):\n",
    "                print(models_names[i])\n",
    "                self.all_models_runs = []   \n",
    "                for n, avg_num in enumerate(self.averages):\n",
    "                    print(avg_num)\n",
    "                    self.all_avg_runs = []\n",
    "                    for j, window_size in enumerate(self.windows):\n",
    "                        print(window_size)\n",
    "                        self.all_round_runs = []\n",
    "                        for l, round in enumerate(range(len(self.subject.train_masks))):\n",
    "                            self.all_split_runs = []\n",
    "                            print(f'Starting round number {l+1} out of {len(self.subject.train_masks)}')\n",
    "                            for m, split_number in enumerate(range(self.subject.num_splits)): # LOOP OVER ALL SPLITS\n",
    "                                print(f'Starting split number {m+1} out of {self.subject.num_splits}')\n",
    "                                filename = f'Sub_{self.subject.subject_id[0]}_{self.subject.lab}_{self.models_names[i]}_{self.description}_round_{l}_{window_size}_step_{self.steps[j]}_{self.subject.num_splits}fold_{avg_num}avg'\n",
    "\n",
    "                                start = timer()\n",
    "                                    \n",
    "                                # Example of working code\n",
    "                                #this_run = This_Run(subject=all_data, split_number = 0, y_variable = all_data.groups, step=step_size[0], window = window_sizes[0], average=avgs[0], model=models[0], percent=percent, mods = mods)\n",
    "                                #this_run.moving_window_subsample()    \n",
    "                                this_run = This_Run(subject=copy.deepcopy(self.subject), split_number = m,round_number = l, y_variable = self.y_variable, step=self.steps[j], window = window_size, average=avg_num, model=model, percent=self.percent, mods = self.description)\n",
    "                                # START RUN\n",
    "                                this_run = this_run.moving_window_subsample_par()\n",
    "                                # CLEAN DATA FROM RUN\n",
    "                                this_run.subject.data = []\n",
    "                                #print(f\"Saving Run {m} Time:\", timer()-start) \n",
    "                                # SAVE RUN\n",
    "                                #current_datetime = datetime.datetime.now()\n",
    "                                #formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%Hh-%Mm-%Ss\")\n",
    "                                #with open(f'Run number {m}{filename}_{formatted_datetime}.pkl', 'wb') as f:\n",
    "                                #    pickle.dump(this_run, f)\n",
    "                                # Store this_run in self\n",
    "                                self.all_split_runs.append(this_run)\n",
    "\n",
    "                                print(f\"Saved Run {m} Time:\", timer()-start) \n",
    "                            current_datetime = datetime.datetime.now()\n",
    "                            formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%Hh-%Mm-%Ss\")\n",
    "                        \n",
    "                            #print_grandavg([self.all_split_runs])\n",
    "\n",
    "                            with open(f'{filename}_{formatted_datetime}.pkl', 'wb') as f:\n",
    "                                pickle.dump(self.all_split_runs, f)\n",
    "                            \n",
    "                            \n",
    "                            self.all_round_runs.append(self.all_split_runs)\n",
    "\n",
    "                        #print_pipeline(self.all_split_runs)\n",
    "                        #self.all_model_runs.append(all_split_runs)\n",
    "                        self.all_avg_runs.append(self.all_split_runs)\n",
    "                    self.all_models_runs.append(self.all_avg_runs)\n",
    "                self.model_outputs.append(self.all_models_runs)\n",
    "\n",
    "                ## Get the current date and time\n",
    "                #current_datetime = datetime.datetime.now()\n",
    "\n",
    "                ## Format the date and time as a string\n",
    "                #formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%Hh-%Mm-%Ss\")\n",
    "                #with open(f'complete_run_{filename}_{formatted_datetime}.pkl', 'wb') as f:\n",
    "                #    pickle.dump(self.model_outputs, f)\n",
    "\n",
    "        #return self\n",
    "\n",
    "        except:\n",
    "            return self\n",
    "\n",
    "def print_pipeline(splits):\n",
    "    \"\"\"\n",
    "    Function to plot the average scores over time, along with error bars and binomial probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - step_size: Step size for time windows.\n",
    "    - splits: List containing three 3D matrices [splits, rounds, scores] for face, house, and noise.\n",
    "    - window_size: Window size for time windows.\n",
    "\n",
    "    Returns:\n",
    "    None (plots the graph and prints binomial probabilities).\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect the auc and CI info from each run\n",
    "    aucs_array = np.array([[np.array(inner_list.score) for inner_list in row] for row in splits])\n",
    "    cis_array = np.array([[np.array(inner_list.cis) for inner_list in row] for row in splits])\n",
    "\n",
    "    y_preds = np.array([[np.array(inner_list.predictions) for inner_list in row] for row in splits])\n",
    "    y_tests = np.array([[np.array(inner_list.y_tests) for inner_list in row] for row in splits])\n",
    "\n",
    "\n",
    "    # roc_results = roc_auc_score(test_y,\n",
    "    #                         y_pred,\n",
    "    #                         confidence_level=0.95)\n",
    "\n",
    "    # # Perform boostrapping for each AUC value\n",
    "    min_cis = np.zeros([aucs_array.shape[1],aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    max_cis = np.zeros([aucs_array.shape[1],aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    #p_values = np.zeros_like(heatmap_data)\n",
    "    bootstrap_aucs = []\n",
    "\n",
    "    num_bootstrap_samples  = 1000\n",
    "    bootstrap_samples = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "    # for i in range(aucs_array.shape[1]):\n",
    "    #     for j in range(aucs_array.shape[2]):\n",
    "    #         for k in range(aucs_array.shape[3]):\n",
    "    #             for l in range(aucs_array.shape[4]):\n",
    "    #                 #y_pred = y_preds[:,i,j,k,l].reshape\n",
    "    #                 #y_test = y_tests[:,i,j,k,l].reshape\n",
    "    #                 #roc_results = roc_auc_score(y_test,\n",
    "    #                 #        y_pred,\n",
    "    #                 #        confidence_level=0.95)\n",
    "                    \n",
    "    #                 for _ in range(num_bootstrap_samples):\n",
    "    #                             #subj, splits, trains, values, test, values \n",
    "    #                     auc_value = np.squeeze(aucs_array[:,i,j,k,l])\n",
    "    #                     #auc_value = auc_value.reshape(auc_value.shape[0]*auc_value.shape[1])\n",
    "    #                     #print(auc_value.shape)\n",
    "    #                     # Perform one-sample t-test against chance level\n",
    "    #                     bootstrap_samples = np.random.choice(auc_value, size=auc_value.shape[0], replace=True)\n",
    "    #                     # Calculate the mean of the bootstrap sample and append it to bootstrap_means\n",
    "    #                     bootstrap_aucs.append(np.mean(bootstrap_samples))\n",
    "    #                     # Calculate confidence interval\n",
    "    #                 ci_lower, ci_upper = np.percentile(bootstrap_samples, [2.5, 97.5])\n",
    "    #                 min_cis[i,j,k,l] = ci_lower\n",
    "    #                 max_cis[i,j,k,l] = ci_upper\n",
    "    #aucs_array = np.array([run.aucs for run in splits])\n",
    "    #cis_array = np.array([run.cis for run in splits])\n",
    "    # Calculate averages and standard errors for each time window  \n",
    "    avg_scores = np.mean(aucs_array, axis=0) # MEAN OVER SPLITS\n",
    "    \n",
    "    avg_cis = np.mean(np.array(cis_array), axis=0)\n",
    "    min_cis = np.min(np.array(cis_array), axis=0).squeeze(axis=0)\n",
    "    max_cis = np.max(np.array(cis_array), axis=0).squeeze(axis=0)\n",
    "    print('max cis',max_cis.shape)\n",
    "\n",
    "    # Create a line plot\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    clrs = sns.color_palette(\"husl\", 5)\n",
    "    clrs = ['b','g','orange']\n",
    "    \n",
    "    #lines = ['-','-','-']\n",
    "    lscores = []\n",
    "    lerrors = []\n",
    "    # Get participant's information\n",
    "    labels = [label.pipeline_labels for label in splits[0]]\n",
    "    \n",
    "    #data_length = splits[0][0].subject.data.shape[2]\n",
    "    window_size = splits[0][0].window\n",
    "    step_size = splits[0][0].step\n",
    "\n",
    "    data_length = (avg_scores.shape[-1] - 1) * step_size + window_size\n",
    "    print(data_length)\n",
    "\n",
    "    num_of_tests = avg_scores.shape[0] # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    num_of_trains = avg_scores.shape[2] # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    print('num of trains', avg_scores.shape[0])\n",
    "    print('num of tests', avg_scores.shape[2])\n",
    "\n",
    "    fig = plt.figure(figsize=(num_of_trains*4, num_of_tests*4))\n",
    "\n",
    "    # NEEDS TO BE CORRECTED FOR 512 collections\n",
    "    time_windows = [i for i in range(0, (data_length * 2) - (window_size * 2), step_size * 2)]\n",
    "    # Create a new array with NaN values\n",
    "    #new_time_windows = np.full_like(time_windows, np.nan)\n",
    "\n",
    "    # Update every second and third value with the original time windows\n",
    "    #new_time_windows[::3] = time_windows[::3]\n",
    "    #time_windows1 = time_windows[::4]\n",
    "\n",
    "    colors = [(0, 1, 0), (0, 0, 1), (1, 1, 1), (1, 0, 0), (1, 1, 0)]  # Blue, White, Red, yellow\n",
    "    #colors = [(0, 0, 1),(0.7, 0.7, 1), (1, 1, 1), (1, .7, .7),(1, 0, 0)]  # Blue, White, Red, yellow\n",
    "    cmap = LinearSegmentedColormap.from_list(\"Custom_RdBu\", colors)\n",
    "    \n",
    "    # Define custom smoothing kernel (ignore diagonals)\n",
    "    kernel = np.array([[1, 1, 0],\n",
    "                    [1, 2, 1],\n",
    "                    [0, 1, 1]])\n",
    "    \n",
    "    # Plot the averages with error bars\n",
    "\n",
    "    count = 1\n",
    "    print(avg_scores.shape)\n",
    "    \n",
    "    for i in range(num_of_trains):\n",
    "        for l in range(num_of_tests):\n",
    "            \n",
    "            ax = plt.subplot(num_of_tests,num_of_trains,count)\n",
    "            \n",
    "        #if count < 4:\n",
    "            #print(labels)\n",
    "            #start_index = labels[l][i].find(\"Train\")\n",
    "            #test_title = labels[l][i][start_index:len(labels[l][i])].strip()\n",
    "            #print(test_title)\n",
    "            \n",
    "            ax.set_title(labels[l][i], loc='center', fontsize=16, y=1, x = .5)\n",
    "\n",
    "            heatmap_data = avg_scores[l,:,i,:]\n",
    "            min_cis_data = min_cis[l,:,i,:]\n",
    "            max_cis_data = max_cis[l,:,i,:]\n",
    "\n",
    "             # Normalize kernel\n",
    "            kernel = kernel / np.sum(kernel)\n",
    "\n",
    "            # Perform 2D convolution with the custom kernel\n",
    "            smoothed_data    = convolve2d(heatmap_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_min_cis = convolve2d(min_cis_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_max_cis = convolve2d(max_cis_data, kernel, mode='same', boundary='fill', fillvalue=0) \n",
    "\n",
    "            pos_data = np.where(smoothed_min_cis > 0.5, 1, 0)\n",
    "            neg_data = np.where(smoothed_max_cis < 0.5, 1, 0)\n",
    "\n",
    "            final_data = pos_data + neg_data\n",
    "\n",
    "            #smoothed_data = smoothed_data * final_data\n",
    "            # SHOW AND HIDE CBAR\n",
    "            cbar_bool = True\n",
    "            ax2 = sns.heatmap(smoothed_data, vmin = 0, vmax = 1, xticklabels = time_windows, yticklabels = time_windows, cmap = cmap,annot=False, square=True, cbar = True)\n",
    "            original_position = ax.get_position()\n",
    "            if count != 1:\n",
    "                cbar_bool = False\n",
    "                #sns.cbar = cbar_bool\n",
    "                cbar = ax.collections[0].colorbar\n",
    "                \n",
    "                cbar.remove()\n",
    "                ax.set_position(original_position)\n",
    "            # HIDE TICKS\n",
    "            show_every = 12\n",
    "            for i, label in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().xaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            for i, label in enumerate(plt.gca().yaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().yaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            # Plot your heatmap here, and set the x-axis ticks to the new time windows\n",
    "            #plt.xticks(range(len(new_time_windows)), new_time_windows)\n",
    "            plt.contour(final_data, colors='black', linestyles='dotted', linewidths=.5, alpha = 0.5)\n",
    "            #sns.set_palette(\"RdBu\")\n",
    "            count = count + 1\n",
    "            start_index = labels[l][0].find(\"Train\")\n",
    "            end_index = labels[l][0].find(\"Test\")\n",
    "            #stim = find_word_stim(title)\n",
    "            between_train_and_iteration = labels[l][0][start_index:end_index].strip()\n",
    "            new_title = f\"Average AUCs, {between_train_and_iteration}\"\n",
    "            \n",
    "            plt.gca().invert_yaxis()\n",
    "            # Show the plot\n",
    "            # PLOT DIAGNOAL\n",
    "            plt.plot([0, avg_scores.shape[3]], [0, avg_scores.shape[1]], color='black', linestyle='-', linewidth=.5, alpha = 0.3)\n",
    "            plt.grid(False)\n",
    "    \n",
    "    # Creates a new empty subplot emcompassing the whole figure (111)\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axis\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    # Adds the labels to this big new subplot\n",
    "    plt.xlabel('Test', fontsize=18)\n",
    "    plt.ylabel('Train', fontsize=18)\n",
    "\n",
    "    # Annotate the plot with subject information\n",
    "    sub_info = splits[0][0].subject\n",
    "    subtitle = f'Sub. {sub_info.subject}, Lab {sub_info.lab}, Model {splits[0][0].model.__class__.__name__}, Average of {splits[0][0].average} Window length {window_size}, Step size {step_size}'\n",
    "\n",
    "# Add a square at the top right of the plot containing the subject information\n",
    "#plt.text(0.5, 1.1, info_text, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "# legs = ''\n",
    "    \n",
    "    plt.suptitle('\\n'+ new_title, fontsize=16,y=1.10)\n",
    "    plt.figtext(0.5, 0.95, subtitle, ha='center', va='center')\n",
    "    #filename = f'{models_name[i]}_{mods}_{window_size}_step_{step_size}_{formatted_datetime}_{num_splits}fold_{avg_num}avg'\n",
    "    plt.savefig( subtitle + new_title + 'subplots' + '.png', format='png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# print_time_window_three_new(step_size, splits_data, window_size)\n",
    "\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def print_timebytime(splits):\n",
    "    \"\"\"\n",
    "    Function to plot the average scores over time, along with error bars and binomial probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - step_size: Step size for time windows.\n",
    "    - splits: List containing three 3D matrices [splits, rounds, scores] for face, house, and noise.\n",
    "    - window_size: Window size for time windows.\n",
    "\n",
    "    Returns:\n",
    "    None (plots the graph and prints binomial probabilities).\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect the auc and CI info from each run\n",
    "    aucs_array = np.array([[np.array(inner_list.aucs) for inner_list in row] for row in splits])\n",
    "    cis_array = np.array([[np.array(inner_list.cis) for inner_list in row] for row in splits])\n",
    "\n",
    "    y_preds = np.array([[np.array(inner_list.predictions) for inner_list in row] for row in splits])\n",
    "    y_tests = np.array([[np.array(inner_list.y_tests) for inner_list in row] for row in splits])\n",
    "\n",
    "\n",
    "    # roc_results = roc_auc_score(test_y,\n",
    "    #                         y_pred,\n",
    "    #                         confidence_level=0.95)\n",
    "\n",
    "    # # Perform boostrapping for each AUC value\n",
    "    min_cis = np.zeros([aucs_array.shape[1],aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    max_cis = np.zeros([aucs_array.shape[1],aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    #p_values = np.zeros_like(heatmap_data)\n",
    "    bootstrap_aucs = []\n",
    "\n",
    "    num_bootstrap_samples  = 1000\n",
    "    bootstrap_samples = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "    # for i in range(aucs_array.shape[1]):\n",
    "    #     for j in range(aucs_array.shape[2]):\n",
    "    #         for k in range(aucs_array.shape[3]):\n",
    "    #             for l in range(aucs_array.shape[4]):\n",
    "    #                 #y_pred = y_preds[:,i,j,k,l].reshape\n",
    "    #                 #y_test = y_tests[:,i,j,k,l].reshape\n",
    "    #                 #roc_results = roc_auc_score(y_test,\n",
    "    #                 #        y_pred,\n",
    "    #                 #        confidence_level=0.95)\n",
    "                    \n",
    "    #                 for _ in range(num_bootstrap_samples):\n",
    "    #                             #subj, splits, trains, values, test, values \n",
    "    #                     auc_value = np.squeeze(aucs_array[:,i,j,k,l])\n",
    "    #                     #auc_value = auc_value.reshape(auc_value.shape[0]*auc_value.shape[1])\n",
    "    #                     #print(auc_value.shape)\n",
    "    #                     # Perform one-sample t-test against chance level\n",
    "    #                     bootstrap_samples = np.random.choice(auc_value, size=auc_value.shape[0], replace=True)\n",
    "    #                     # Calculate the mean of the bootstrap sample and append it to bootstrap_means\n",
    "    #                     bootstrap_aucs.append(np.mean(bootstrap_samples))\n",
    "    #                     # Calculate confidence interval\n",
    "    #                 ci_lower, ci_upper = np.percentile(bootstrap_samples, [2.5, 97.5])\n",
    "    #                 min_cis[i,j,k,l] = ci_lower\n",
    "    #                 max_cis[i,j,k,l] = ci_upper\n",
    "    #aucs_array = np.array([run.aucs for run in splits])\n",
    "    #cis_array = np.array([run.cis for run in splits])\n",
    "    # Calculate averages and standard errors for each time window  \n",
    "    avg_scores = np.mean(aucs_array, axis=0) # MEAN OVER SPLITS\n",
    "    \n",
    "    avg_cis = np.mean(np.array(cis_array), axis=0)\n",
    "    min_cis = np.min(np.array(cis_array), axis=0).squeeze(axis=0)\n",
    "    max_cis = np.max(np.array(cis_array), axis=0).squeeze(axis=0)\n",
    "    print('max cis',max_cis.shape)\n",
    "\n",
    "    # Create a line plot\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    clrs = sns.color_palette(\"husl\", 5)\n",
    "    clrs = ['b','g','orange']\n",
    "    \n",
    "    #lines = ['-','-','-']\n",
    "    lscores = []\n",
    "    lerrors = []\n",
    "    # Get participant's information\n",
    "    labels = [label.pipeline_labels for label in splits[0]]\n",
    "    \n",
    "    #data_length = splits[0][0].subject.data.shape[2]\n",
    "    window_size = splits[0][0].window\n",
    "    step_size = splits[0][0].step\n",
    "\n",
    "    data_length = (avg_scores.shape[-1] - 1) * step_size + window_size\n",
    "    print(data_length)\n",
    "\n",
    "    num_of_tests = avg_scores.shape[0] # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    num_of_trains = avg_scores.shape[2] # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    print('num of trains', avg_scores.shape[0])\n",
    "    print('num of tests', avg_scores.shape[2])\n",
    "\n",
    "    fig = plt.figure(figsize=(num_of_trains*4, num_of_tests*4))\n",
    "\n",
    "    # NEEDS TO BE CORRECTED FOR 512 collections\n",
    "    time_windows = [i - 60 for i in range(0, (data_length * 2) - (window_size * 2), step_size * 2)]\n",
    "    # Create a new array with NaN values\n",
    "    #new_time_windows = np.full_like(time_windows, np.nan)\n",
    "\n",
    "    # Update every second and third value with the original time windows\n",
    "    #new_time_windows[::3] = time_windows[::3]\n",
    "    #time_windows1 = time_windows[::4]\n",
    "\n",
    "    colors = [(0, 1, 0), (0, 0, 1), (1, 1, 1), (1, 0, 0), (1, 1, 0)]  # Blue, White, Red, yellow\n",
    "    #colors = [(0, 0, 1),(0.7, 0.7, 1), (1, 1, 1), (1, .7, .7),(1, 0, 0)]  # Blue, White, Red, yellow\n",
    "    cmap = LinearSegmentedColormap.from_list(\"Custom_RdBu\", colors)\n",
    "    \n",
    "    # Define custom smoothing kernel (ignore diagonals)\n",
    "    kernel = np.array([[1, 1, 0],\n",
    "                    [1, 2, 1],\n",
    "                    [0, 1, 1]])\n",
    "    \n",
    "    # Plot the averages with error bars\n",
    "\n",
    "    count = 1\n",
    "    print(avg_scores.shape)\n",
    "    \n",
    "    for i in range(num_of_trains):\n",
    "        for l in range(num_of_tests):\n",
    "            \n",
    "            ax = plt.subplot(num_of_tests,num_of_trains,count)\n",
    "            \n",
    "        #if count < 4:\n",
    "            #print(labels)\n",
    "            #start_index = labels[l][i].find(\"Train\")\n",
    "            #test_title = labels[l][i][start_index:len(labels[l][i])].strip()\n",
    "            #print(test_title)\n",
    "            \n",
    "            ax.set_title(labels[l][i], loc='center', fontsize=16, y=1, x = .5)\n",
    "\n",
    "            heatmap_data = avg_scores[l,:,i,:]\n",
    "            min_cis_data = min_cis[l,:,i,:]\n",
    "            max_cis_data = max_cis[l,:,i,:]\n",
    "\n",
    "             # Normalize kernel\n",
    "            kernel = kernel / np.sum(kernel)\n",
    "\n",
    "            # Perform 2D convolution with the custom kernel\n",
    "            smoothed_data    = convolve2d(heatmap_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_min_cis = convolve2d(min_cis_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_max_cis = convolve2d(max_cis_data, kernel, mode='same', boundary='fill', fillvalue=0) \n",
    "\n",
    "            pos_data = np.where(smoothed_min_cis > 0.5, 1, 0)\n",
    "            neg_data = np.where(smoothed_max_cis < 0.5, 1, 0)\n",
    "\n",
    "            final_data = pos_data + neg_data\n",
    "\n",
    "            #smoothed_data = smoothed_data * final_data\n",
    "            # SHOW AND HIDE CBAR\n",
    "            cbar_bool = True\n",
    "            ax2 = sns.heatmap(smoothed_data, vmin = 0, vmax = 1, xticklabels = time_windows, yticklabels = time_windows, cmap = cmap,annot=False, square=True, cbar = True)\n",
    "            original_position = ax.get_position()\n",
    "            if count != 1:\n",
    "                cbar_bool = False\n",
    "                #sns.cbar = cbar_bool\n",
    "                cbar = ax.collections[0].colorbar\n",
    "                \n",
    "                cbar.remove()\n",
    "                ax.set_position(original_position)\n",
    "            # HIDE TICKS\n",
    "            show_every = 12\n",
    "            for i, label in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().xaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            for i, label in enumerate(plt.gca().yaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().yaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            # Plot your heatmap here, and set the x-axis ticks to the new time windows\n",
    "            #plt.xticks(range(len(new_time_windows)), new_time_windows)\n",
    "            plt.contour(final_data, colors='black', linestyles='dotted', linewidths=.5, alpha = 0.5)\n",
    "            #sns.set_palette(\"RdBu\")\n",
    "            count = count + 1\n",
    "            start_index = labels[l][0].find(\"Train\")\n",
    "            end_index = labels[l][0].find(\"Test\")\n",
    "            #stim = find_word_stim(title)\n",
    "            between_train_and_iteration = labels[l][0][start_index:end_index].strip()\n",
    "            new_title = f\"Average AUCs, {between_train_and_iteration}\"\n",
    "            \n",
    "            plt.gca().invert_yaxis()\n",
    "            # Show the plot\n",
    "            # PLOT DIAGNOAL\n",
    "            plt.plot([0, avg_scores.shape[3]], [0, avg_scores.shape[1]], color='black', linestyle='-', linewidth=.5, alpha = 0.3)\n",
    "            plt.grid(False)\n",
    "    \n",
    "    # Creates a new empty subplot emcompassing the whole figure (111)\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axis\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    # Adds the labels to this big new subplot\n",
    "    plt.xlabel('Test', fontsize=18)\n",
    "    plt.ylabel('Train', fontsize=18)\n",
    "\n",
    "    # Annotate the plot with subject information\n",
    "    sub_info = splits[0][0].subject\n",
    "    subtitle = f'Sub. {sub_info.subject}, Lab {sub_info.lab}, Model {splits[0][0].model.__class__.__name__}, Average of {splits[0][0].average} Window length {window_size}, Step size {step_size}'\n",
    "\n",
    "# Add a square at the top right of the plot containing the subject information\n",
    "#plt.text(0.5, 1.1, info_text, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "# legs = ''\n",
    "    \n",
    "    plt.suptitle('\\n'+ new_title, fontsize=16)\n",
    "    plt.figtext(0.5, 0.915, subtitle, ha='center', va='center')\n",
    "    #filename = f'{models_name[i]}_{mods}_{window_size}_step_{step_size}_{formatted_datetime}_{num_splits}fold_{avg_num}avg'\n",
    "    plt.savefig( subtitle + new_title + 'subplots' + '.png', format='png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# print_time_window_three_new(step_size, splits_data, window_size)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def print_timebytime_topoplots(splits):\n",
    "    \"\"\"\n",
    "    Function to plot the average scores over time, along with error bars and binomial probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - step_size: Step size for time windows.\n",
    "    - splits: List containing three 3D matrices [splits, rounds, scores] for face, house, and noise.\n",
    "    - window_size: Window size for time windows.\n",
    "\n",
    "    Returns:\n",
    "    None (plots the graph and prints binomial probabilities).\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect the auc and CI info from each run\n",
    "    aucs_array = np.array([[np.array(inner_list.aucs) for inner_list in row] for row in splits])\n",
    "    cis_array = np.array([[np.array(inner_list.cis) for inner_list in row] for row in splits])\n",
    "    #aucs_array = np.array([run.aucs for run in splits])\n",
    "    #cis_array = np.array([run.cis for run in splits])\n",
    "    # Calculate averages and standard errors for each time window  \n",
    "    avg_scores = np.mean(aucs_array, axis=0) # MEAN OVER SPLITS\n",
    "    \n",
    "    # # Perform one-sample t-test for each AUC value\n",
    "    min_cis = np.zeros([aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    max_cis = np.zeros([aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    #p_values = np.zeros_like(heatmap_data)\n",
    "    bootstrap_aucs = []\n",
    "\n",
    "    for i in range(aucs_array.shape[1]):\n",
    "        for j in range(aucs_array.shape[2]):\n",
    "            for k in range(aucs_array.shape[3]):\n",
    "                for l in range(aucs_array.shape[4]):\n",
    "                            #subj, splits, trains, values, test, values \n",
    "                    auc_value = np.squeeze(aucs_array[:,i,j,k,l])\n",
    "                    auc_value = auc_value.reshape(auc_value.shape[0]*auc_value.shape[1])\n",
    "                    #print(auc_value.shape)\n",
    "                    # Perform one-sample t-test against chance level\n",
    "                    bootstrap_sample = np.random.choice(auc_value, size=auc_value.shape[0], replace=True)\n",
    "                    # Calculate the mean of the bootstrap sample and append it to bootstrap_means\n",
    "                    bootstrap_aucs.append(np.mean(bootstrap_sample))\n",
    "                    # Calculate confidence interval\n",
    "                    ci_lower, ci_upper = np.percentile(bootstrap_sample, [0.0025, 99.9975])\n",
    "                    min_cis[i,j,k,l] = ci_lower\n",
    "                    max_cis[i,j,k,l] = ci_upper\n",
    "\n",
    "    avg_cis = np.mean(np.array(cis_array), axis=0)\n",
    "    min_cis = np.min(np.array(cis_array), axis=0)\n",
    "    max_cis = np.max(np.array(cis_array), axis=0)\n",
    "\n",
    "    # Create a line plot\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    clrs = sns.color_palette(\"husl\", 5)\n",
    "    clrs = ['b','g','orange']\n",
    "    \n",
    "    #lines = ['-','-','-']\n",
    "    lscores = []\n",
    "    lerrors = []\n",
    "    # Get participant's information\n",
    "    labels = [label.pipeline_labels for label in splits[0]]\n",
    "    \n",
    "    #data_length = splits[0][0].subject.data.shape[2]\n",
    "    window_size = splits[0][0].window\n",
    "    step_size = splits[0][0].step\n",
    "\n",
    "    data_length = (avg_scores.shape[-1] - 1) * step_size + window_size\n",
    "    print(data_length)\n",
    "    num_or_tests = 3 # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    num_of_trains = 3 # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "\n",
    "\n",
    "    # NEEDS TO BE CORRECTED FOR 512 collections\n",
    "    time_windows = [i - 60 for i in range(0, (data_length * 2) - (window_size * 2), step_size * 2)]\n",
    "    # Create a new array with NaN values\n",
    "    #new_time_windows = np.full_like(time_windows, np.nan)\n",
    "\n",
    "    # Update every second and third value with the original time windows\n",
    "    #new_time_windows[::3] = time_windows[::3]\n",
    "    #time_windows1 = time_windows[::4]\n",
    "\n",
    "    colors = [(0, 1, 0), (0, 0, 1), (1, 1, 1), (1, 0, 0), (1, 1, 0)]  # Blue, White, Red, yellow\n",
    "    #colors = [(0, 0, 1),(0.7, 0.7, 1), (1, 1, 1), (1, .7, .7),(1, 0, 0)]  # Blue, White, Red, yellow\n",
    "    cmap = LinearSegmentedColormap.from_list(\"Custom_RdBu\", colors)\n",
    "    \n",
    "    # Define custom smoothing kernel (ignore diagonals)\n",
    "    kernel = np.array([[1, 1, 0],\n",
    "                    [1, 2, 1],\n",
    "                    [0, 1, 1]])\n",
    "    \n",
    "    # Plot the averages with error bars\n",
    "\n",
    "    count = 1\n",
    "    print(avg_scores.shape)\n",
    "    \n",
    "    for l in range(3):\n",
    "        for i in range(3):\n",
    "            \n",
    "            ax = plt.subplot(3,3,count)\n",
    "            \n",
    "        #if count < 4:\n",
    "            #print(labels)\n",
    "            start_index = labels[i][l].find(\"Train\")\n",
    "            test_title = labels[i][l][start_index:len(labels[l][i])].strip()\n",
    "            #print(test_title)\n",
    "            \n",
    "            ax.set_title(labels[i][l], loc='center', fontsize=16, y=1, x = .5)\n",
    "\n",
    "            heatmap_data = avg_scores[l,:,i,:]\n",
    "            min_cis_data = min_cis[l,:,i,:,0]\n",
    "            max_cis_data = max_cis[l,:,i,:,1]\n",
    "\n",
    "             # Normalize kernel\n",
    "            kernel = kernel / np.sum(kernel)\n",
    "\n",
    "            # Perform 2D convolution with the custom kernel\n",
    "            smoothed_data    = convolve2d(heatmap_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_min_cis = convolve2d(min_cis_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_max_cis = convolve2d(max_cis_data, kernel, mode='same', boundary='fill', fillvalue=0) \n",
    "\n",
    "            pos_data = np.where(smoothed_min_cis > 0.5, 1, 0)\n",
    "            neg_data = np.where(smoothed_max_cis < 0.5, 1, 0)\n",
    "\n",
    "            final_data = pos_data + neg_data\n",
    "\n",
    "            #smoothed_data = smoothed_data * final_data\n",
    "            # SHOW AND HIDE CBAR\n",
    "            cbar_bool = True\n",
    "            #ax2 = sns.heatmap(smoothed_data, vmin = 0, vmax = 1, xticklabels = time_windows, yticklabels = time_windows, cmap = cmap,annot=False, square=True, cbar = True)\n",
    "            print_mean_coeff(avg_coefs[i,j],time_windows[j])\n",
    "            original_position = ax.get_position()\n",
    "            if count != 1:\n",
    "                cbar_bool = False\n",
    "                #sns.cbar = cbar_bool\n",
    "                cbar = ax.collections[0].colorbar\n",
    "                \n",
    "                cbar.remove()\n",
    "                ax.set_position(original_position)\n",
    "            # HIDE TICKS\n",
    "            show_every = 12\n",
    "            for i, label in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().xaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            for i, label in enumerate(plt.gca().yaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().yaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            # Plot your heatmap here, and set the x-axis ticks to the new time windows\n",
    "            #plt.xticks(range(len(new_time_windows)), new_time_windows)\n",
    "            plt.contour(final_data, colors='black', linestyles='dotted', linewidths=.5, alpha = 0.5)\n",
    "            #sns.set_palette(\"RdBu\")\n",
    "            count = count + 1\n",
    "            start_index = labels[l][0].find(\"Train\")\n",
    "            end_index = labels[l][0].find(\"Test\")\n",
    "            #stim = find_word_stim(title)\n",
    "            between_train_and_iteration = labels[l][0][start_index:end_index].strip()\n",
    "            new_title = f\"Average AUCs, {between_train_and_iteration}\"\n",
    "            \n",
    "            plt.gca().invert_yaxis()\n",
    "            # Show the plot\n",
    "            # PLOT DIAGNOAL\n",
    "            plt.plot([0, avg_scores.shape[3]], [0, avg_scores.shape[1]], color='black', linestyle='-', linewidth=.5, alpha = 0.3)\n",
    "            plt.grid(False)\n",
    "    \n",
    "    # Creates a new empty subplot emcompassing the whole figure (111)\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axis\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    # Adds the labels to this big new subplot\n",
    "    plt.xlabel('Test', fontsize=18)\n",
    "    plt.ylabel('Train', fontsize=18)\n",
    "\n",
    "    # Annotate the plot with subject information\n",
    "    sub_info = splits[0][0].subject\n",
    "    subtitle = f'Sub. {sub_info.subject}, Lab {sub_info.lab}, Model {splits[0][0].model.__class__.__name__}, Average of {splits[0][0].average} Window length {window_size}, Step size {step_size}'\n",
    "\n",
    "# Add a square at the top right of the plot containing the subject information\n",
    "#plt.text(0.5, 1.1, info_text, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "# legs = ''\n",
    "    \n",
    "    plt.suptitle('\\n'+ new_title, fontsize=16)\n",
    "    plt.figtext(0.5, 0.915, subtitle, ha='center', va='center')\n",
    "    #filename = f'{models_name[i]}_{mods}_{window_size}_step_{step_size}_{formatted_datetime}_{num_splits}fold_{avg_num}avg'\n",
    "    plt.savefig( subtitle + new_title + 'subplots' + '.png', format='png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# print_time_window_three_new(step_size, splits_data, window_size)\n",
    "\n",
    "\n",
    "# PLOT TOPOMAPS\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_mean_coeff(data,time_point):\n",
    "   # Load electrode locations from a .loc file (replace 'your_electrode_locs.loc' with the actual file path)\n",
    "   montage = mne.channels.read_custom_montage('C:\\GitHub\\Triangulation\\Reed_montage_59.loc')\n",
    "\n",
    "   # Create a data array with values corresponding to each electrode (replace with your actual data)\n",
    "   n_channels = len(montage.ch_names)\n",
    "   n_times = 80  # Number of time points\n",
    "   #data = np.random.rand(n_channels, n_times)  # Replace with your actual EEG data\n",
    "   ##data = estimator_weights_sum.sum(axis = 1)\n",
    "   print(data)\n",
    "   try:\n",
    "      data = data.reshape(59,int(data.shape[1]/59))\n",
    "   except: \n",
    "      data = data.reshape(64,int(data.shape[1]/64))\n",
    "   print(data.shape)\n",
    "   data = np.mean(data,axis=1)\n",
    "\n",
    "   # Create an MNE Info object with channel information and the custom montage\n",
    "   sfreq = 1  # Sampling frequency in Hz\n",
    "   ch_names = montage.ch_names\n",
    "   ch_types = ['eeg'] * n_channels\n",
    "   info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "   info.set_montage(montage)\n",
    "\n",
    "   # Plot a topomap for the data (you can specify the time point if needed)\n",
    "   #time_point = 1  # Time point in seconds (replace with your desired time point)\n",
    "   fig, ax = plt.subplots()\n",
    "   ax.set_title(f'Topomap at {time_point} seconds')\n",
    "   mne.viz.plot_topomap(data, info, axes=ax, cmap='coolwarm', contours=0, show=True)\n",
    "\n",
    "\n",
    "#for column in data.T:\n",
    "    \n",
    "   # time_point = 0.0  # Time point in seconds (replace with your desired time point)\n",
    "   # fig, ax = plt.subplots()\n",
    "   # mne.viz.plot_topomap(column, info, axes=ax, cmap='coolwarm', contours=0, show=True)\n",
    "   # ax.set_title(f'Topomap at {time_point} seconds')\n",
    "# TRAIN ALL TEST INDIVIDUALLY\n",
    "\n",
    "# round robin 1\n",
    "\n",
    "def apply_masks(subj1,y_variable,IB_mask,DC_mask,BM_mask,h_mask,f_mask,n_mask,vis_mask,invis_mask,test_mask,train_mask,experiment2,phase2,stimulus2,split2):\n",
    "    final_data = []\n",
    "    # TRAIN DC\n",
    "\n",
    "    for i in range(subj1.num_splits):\n",
    "        print(f'split number {i}')\n",
    "        final_mask = []\n",
    "        split_data = []\n",
    "        final_test_mask = []\n",
    "        final_test_mask_groups = []\n",
    "        \n",
    "        round_robin = []\n",
    "        round_robins = []\n",
    "\n",
    "        final_mask.append([(f_mask+h_mask)*(train_mask[:,i])])\n",
    "        #print(np.sum(final_mask))\n",
    "        final_test_mask.append([(f_mask+h_mask)*(test_mask[:,i])])\n",
    "        final_test_mask.append([(f_mask)*(test_mask[:,i])])\n",
    "        final_test_mask.append([(h_mask)*(test_mask[:,i])])\n",
    "\n",
    "        round_robin.append('Train ALL Test ALL')\n",
    "        round_robin.append('Train ALL Test Face')\n",
    "        round_robin.append('Train ALL Test House')\n",
    "        final_test_mask_groups.append(final_test_mask)\n",
    "        #print(np.sum(final_test_mask_groups))\n",
    "        round_robins.append(round_robin)\n",
    "        round_robin = []\n",
    "        final_test_mask = []\n",
    "\n",
    "        masked_by_tests = []\n",
    "        masked_data = []\n",
    "        \n",
    "        for j in range(0,len(final_mask)):\n",
    "            print(f'masking number {j}')\n",
    "            final_test_mask_groups_arr = np.array(final_test_mask_groups)\n",
    "\n",
    "            final_mask_arr = np.array(final_mask)\n",
    "            #subj1.groups is the label source, but this is a bit chaotic\n",
    "            split_data.append(mask_data_multipletests(final_mask_arr[j,0],final_test_mask_groups_arr[j,:,0],y_variable,subj1.data,round_robins[j]))\n",
    "\n",
    "        \n",
    "        final_data.append(split_data)\n",
    "        #print(np.array(final_data).shape)\n",
    "        #print('A splits, B trains, C tests, D data and labels')\n",
    "        # final_data[number of splits[number of tests]]\n",
    "    return final_data, round_robins\n",
    "    \n",
    "def mask_data_multipletests(train_mask,test_mask,label_source,data,round_name):\n",
    "    #print(all_data)\n",
    "    #print(train_mask.shape)\n",
    "    #print(np.sum(data))\n",
    "    #print(data)\n",
    "    train_data = data[train_mask,...]\n",
    "    \n",
    "    true_indices = [index for index, value in enumerate(train_mask) if value]\n",
    "    #print(true_indices)\n",
    "    train_labels = label_source[train_mask]\n",
    "    all_test_data = []\n",
    "    all_tests_labels = []\n",
    "    #print(test_mask.shape)\n",
    "    for tests in test_mask:\n",
    "        #print(tests.shape)\n",
    "        #print(any(tests))\n",
    "        test_data = data[tests,...]\n",
    "        test_labels = label_source[tests]\n",
    "\n",
    "        #print(test_data)\n",
    "        test_data,test_labels = shuffle_data(test_data,test_labels)\n",
    "        all_test_data.append(test_data)\n",
    "        #print(test_labels)\n",
    "        all_tests_labels.append(test_labels)\n",
    "\n",
    "    train_data,train_labels = shuffle_data(train_data,train_labels)\n",
    "    #print('all test data', all_test_data)\n",
    "    #print('all test labels', all_tests_labels)\n",
    "\n",
    "    final_data = [(train_data),(train_labels),(all_test_data),(all_tests_labels),round_name]\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def create_masks(experiment,phase,stimulus,split,groups):\n",
    "\n",
    "    IB_mask = (experiment == 'IB')\n",
    "    DC_mask = (experiment == 'DC')\n",
    "    BM_mask = (experiment == 'BM')\n",
    "\n",
    "    h_mask = (stimulus == 'house')\n",
    "    f_mask = (stimulus == 'face')\n",
    "    n_mask = (stimulus == 'noise')\n",
    "\n",
    "    vis_mask = (phase == 'visible')\n",
    "    invis_mask = (phase == 'invisible')\n",
    "\n",
    "    test_mask = (split == 'test')\n",
    "    train_mask = (split == 'train')\n",
    "\n",
    "    blinded_mask = (groups == 1)\n",
    "    not_blinded_mask = (groups == 0)\n",
    "\n",
    "    return IB_mask,DC_mask,BM_mask,h_mask,f_mask,n_mask,vis_mask,invis_mask,test_mask,train_mask,blinded_mask,not_blinded_mask\n",
    "\n",
    "def create_masks_balanced(experiment):\n",
    "\n",
    "    IB_mask = (experiment == 'IB')\n",
    "    DC_mask = (experiment == 'DC')\n",
    "    BM_mask = (experiment == 'BM')\n",
    "\n",
    "    # Find the minimum number of trials among the three experiments\n",
    "    min_num_trials = min(np.sum(IB_mask[0]), np.sum(DC_mask[0]), np.sum(BM_mask[0]))\n",
    "    \n",
    "    # Sample the experiments with fewer trials to match the minimum number\n",
    "    IB_mask_indices = np.where(IB_mask[0])[0]\n",
    "    DC_mask_indices = np.where(DC_mask[0])[0]\n",
    "    BM_mask_indices = np.where(BM_mask[0])[0]\n",
    "\n",
    "    # Select random indices for undersampling\n",
    "    selected_indices_IB = np.random.choice(IB_mask_indices, size=min_num_trials, replace=False)\n",
    "    selected_indices_DC = np.random.choice(DC_mask_indices, size=min_num_trials, replace=False)\n",
    "    selected_indices_BM = np.random.choice(BM_mask_indices, size=min_num_trials, replace=False)\n",
    "\n",
    "    # Reshape them into the original shape\n",
    "    reshaped_IB_indices = np.expand_dims(selected_indices_IB, axis=1)\n",
    "    reshaped_DC_indices = np.expand_dims(selected_indices_DC, axis=1)\n",
    "    reshaped_BM_indices = np.expand_dims(selected_indices_BM, axis=1)\n",
    "\n",
    "    # Create new masks with fewer '1's while maintaining original size\n",
    "    undersampled_IB_mask = np.zeros_like(IB_mask, dtype=bool)\n",
    "    undersampled_IB_mask[selected_indices_IB] = True\n",
    "\n",
    "    undersampled_DC_mask = np.zeros_like(DC_mask, dtype=bool)\n",
    "    undersampled_DC_mask[selected_indices_DC] = True\n",
    "\n",
    "    undersampled_BM_mask = np.zeros_like(BM_mask, dtype=bool)\n",
    "    undersampled_BM_mask[selected_indices_BM] = True\n",
    "\n",
    "    return undersampled_IB_mask,undersampled_DC_mask,undersampled_BM_mask\n",
    "\n",
    "import concurrent.futures\n",
    "import copy\n",
    "\n",
    "def run_pipeline_for_subject(sub, all_data):\n",
    "    print(f'running subject {sub}')\n",
    "    subj_data = copy.deepcopy(all_data)\n",
    "    subj_data.filter_data('subject_id', sub)\n",
    "    print('Subj Data size after filtering',subj_data.data.shape)\n",
    "    subj_data.subject = sub\n",
    "    subj_data.split_data2()\n",
    "    \n",
    "    \n",
    "\n",
    "    subj_data.train_masks = [[['face'],['train']],[['house'],['train']],[['noise'],['train']]]\n",
    "    subj_data.test_masks = [[\n",
    "        [['face'],['test']],\n",
    "        [['house'],['test']],\n",
    "        [['noise'],['test']]\n",
    "    ],[\n",
    "        [['face'],['test']],\n",
    "        [['house'],['test']],\n",
    "        [['noise'],['test']]\n",
    "    ],[\n",
    "        [['face'],['test']],\n",
    "        [['house'],['test']],\n",
    "        [['noise'],['test']]\n",
    "    ]]\n",
    "    subj_data.round_robin_labels = [['Train Face Test Face',\n",
    "                                     'Train Face Test House',\n",
    "                                     'Train Face Test Noise'],\n",
    "                                     ['Train House Test Face',\n",
    "                                     'Train House Test House',\n",
    "                                     'Train House Test Noise'],\n",
    "                                     ['Train Noise Test Face',\n",
    "                                     'Train Noise Test House',\n",
    "                                     'Train Noise Test Noise']]\n",
    "\n",
    "    this_pipeline = This_Pipeline(subject=subj_data, y_variable=subj_data.phase, models=models, models_names=models_names, steps=step_size, averages=avgs, windows=window_sizes, percent=percent, mods=mods)\n",
    "    this_pipe = this_pipeline.run_models()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#len(np.unique(all_data.subject_id))\n",
    "#all_data.data = all_data.data[:,:,500:1000]\n",
    "#\n",
    "#all_data.train_masks = [[['face','house','noise'],['train']]]\n",
    "#all_data.test_masks = [[\n",
    "#    [['face','house','noise'],['train']],\n",
    "#    [['face'],['train']],\n",
    "#    [['house'],['train']],\n",
    "#    [['house'],['train']]\n",
    "#    ]]\n",
    "#\n",
    "#all_data.round_robin_labels = [['Train ALL Test ALL',\n",
    "#                        'Train ALL Test Face',\n",
    "#                        'Train ALL Test House',\n",
    "#                        'Train ALL Test Noise']]\n",
    "# # Use ProcessPoolExecutor to parallelize the innermost loops\n",
    "#with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#    futures = []\n",
    "#for sub in np.unique(all_data.subject_id):\n",
    "#    print(f'running subject {sub}')\n",
    "#    subj_data = copy.deepcopy(all_data)\n",
    "#    subj_data.filter_data('subject_id', sub)\n",
    "#    print('Subj Data size after filtering',subj_data.data.shape)\n",
    "#    subj_data.subject = sub\n",
    "#    subj_data.split_data2()\n",
    "#\n",
    "#    subj_data.train_masks = [[['face','house','noise'],['train']]]\n",
    "#    subj_data.test_masks = [[\n",
    "#    [['face','house','noise'],['train']],\n",
    "#    [['face'],['train']],\n",
    "#    [['house'],['train']],\n",
    "#    [['house'],['train']],\n",
    "#        ]]\n",
    "#    subj_data.round_robin_labels = [['Train ALL Test ALL',\n",
    "#                        'Train ALL Test Face',\n",
    "#                        'Train ALL Test House',\n",
    "#                        'Train ALL Test Noise']]\n",
    "#\n",
    "#    this_pipeline = This_Pipeline(subject=subj_data, y_variable=subj_data.phase, models=models, models_names=models_names, steps=step_size, averages=avgs, windows=window_sizes, percent=percent, mods=mods)\n",
    "#    this_pipeline.run_models()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ERP(data,title):\n",
    "    # Create MNE info\n",
    "    sfreq = 500  # Sampling frequency\n",
    "    montage = mne.channels.read_custom_montage('Reed_HM_59.loc')\n",
    "    n_channels = len(montage.ch_names)\n",
    "    ch_names = montage.ch_names\n",
    "    ch_types = ['eeg'] * n_channels\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    info.set_montage(montage)\n",
    "\n",
    "    chan_mask = np.full(64,True)\n",
    "    chan_mask[58] = False\n",
    "    chan_mask[60:64] = False\n",
    "    \n",
    "    evoked = mne.EvokedArray(data[chan_mask,:], info, tmin=0)\n",
    "    fig = evoked.plot(window_title =title)\n",
    "\n",
    "    fig.savefig(f'{title}.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def print_grandavg(splits):\n",
    "\n",
    "    # Collect the auc and CI info from each run\n",
    "    aucs_array = np.array([[np.array(inner_list.aucs) for inner_list in row] for row in splits])\n",
    "    cis_array = np.array([[np.array(inner_list.cis) for inner_list in row] for row in splits])\n",
    "\n",
    "    #y_preds = np.array([[np.array(inner_list.predictions) for inner_list in row] for row in splits])\n",
    "    #y_tests = np.array([[np.array(inner_list.y_tests) for inner_list in row] for row in splits])\n",
    "\n",
    "    # # Perform boostrapping for each AUC value\n",
    "    print(aucs_array.shape)\n",
    "    min_cis = np.zeros([aucs_array.shape[1],aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    max_cis = np.zeros([aucs_array.shape[1],aucs_array.shape[2],aucs_array.shape[3],aucs_array.shape[4]])\n",
    "    #p_values = np.zeros_like(heatmap_data)\n",
    "    bootstrap_aucs = []\n",
    "\n",
    "    num_bootstrap_samples  = 1000\n",
    "    bootstrap_samples = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "    # Calculate averages and standard errors for each time window  \n",
    "    #avg_scores = np.mean(aucs_array[0], axis=0) # MEAN OVER SPLITS\n",
    "    avg_scores = np.mean(np.mean(aucs_array, axis=1), axis=0)  # MEAN OVER SPLITS AND THEN GRAND AVERAGE\n",
    "\n",
    "    print('avg scores shape',avg_scores.shape)\n",
    "    \n",
    "    avg_cis = np.mean(np.array(cis_array), axis=0)\n",
    "    print(cis_array.shape)\n",
    "    min_cis = np.min(np.min(np.array(cis_array[...,0]), axis=0),axis=0)\n",
    "    max_cis = np.max(np.max(np.array(cis_array[...,1]), axis=0),axis=0)\n",
    "    print('max cis',max_cis.shape)\n",
    "\n",
    "    # Create a line plot\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    clrs = sns.color_palette(\"husl\", 5)\n",
    "    clrs = ['b','g','orange']\n",
    "    \n",
    "    #lines = ['-','-','-']\n",
    "    lscores = []\n",
    "    lerrors = []\n",
    "    # Get participant's information\n",
    "    labels = [label.pipeline_labels[splits[0][0].round_number] for label in splits[0]]\n",
    "    print(labels)\n",
    "    #print(labels.shape)\n",
    "    \n",
    "    #data_length = splits[0][0].subject.data.shape[2]\n",
    "    window_size = splits[0][0].window\n",
    "    step_size = splits[0][0].step\n",
    "\n",
    "    data_length = (avg_scores.shape[-1] - 1) * step_size + window_size\n",
    "    print(data_length)\n",
    "\n",
    "    num_of_tests = avg_scores.shape[1] # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    num_of_rounds = 1 # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    print('num of rounds', 1)\n",
    "    print('num of tests', avg_scores.shape[1])\n",
    "\n",
    "    fig = plt.figure(figsize=(num_of_tests*4,num_of_rounds*4))\n",
    "\n",
    "    # NEEDS TO BE CORRECTED FOR 512 collections\n",
    "    time_windows = [i for i in range(0, (data_length * 2) - (window_size * 2), step_size * 2)]\n",
    "    # Create a new array with NaN values\n",
    "    #new_time_windows = np.full_like(time_windows, np.nan)\n",
    "\n",
    "    # Update every second and third value with the original time windows\n",
    "    #new_time_windows[::3] = time_windows[::3]\n",
    "    #time_windows1 = time_windows[::4]\n",
    "\n",
    "    colors = [(0, 1, 0), (0, 0, 1), (1, 1, 1), (1, 0, 0), (1, 1, 0)]  # Blue, White, Red, yellow\n",
    "    #colors = [(0, 0, 1),(0.7, 0.7, 1), (1, 1, 1), (1, .7, .7),(1, 0, 0)]  # Blue, White, Red, yellow\n",
    "    cmap = LinearSegmentedColormap.from_list(\"Custom_RdBu\", colors)\n",
    "    \n",
    "    # Define custom smoothing kernel (ignore diagonals)\n",
    "    kernel = np.array([[1, 1, 0],\n",
    "                    [1, 2, 1],\n",
    "                    [0, 1, 1]])\n",
    "    \n",
    "    # Plot the averages with error bars\n",
    "\n",
    "    count = 1\n",
    "    print(avg_scores.shape)\n",
    "    \n",
    "    \n",
    "    for i in range(num_of_rounds):\n",
    "        for l in range(num_of_tests):\n",
    "            print('l', l)\n",
    "            ax = plt.subplot(num_of_rounds,num_of_tests,count)\n",
    "            \n",
    "        #if count < 4:\n",
    "            #print(labels)\n",
    "            #start_index = labels[l][i].find(\"Train\")\n",
    "            #test_title = labels[l][i][start_index:len(labels[l][i])].strip()\n",
    "            #print(test_title)\n",
    "            \n",
    "            ax.set_title(labels[0][l], loc='center', fontsize=16, y=1, x = .5)\n",
    "\n",
    "            heatmap_data = avg_scores[:,l,:]\n",
    "            print(min_cis.shape)\n",
    "            min_cis_data = min_cis[:,l,:]\n",
    "            max_cis_data = max_cis[:,l,:]\n",
    "\n",
    "             # Normalize kernel\n",
    "            kernel = kernel / np.sum(kernel)\n",
    "\n",
    "            # Perform 2D convolution with the custom kernel\n",
    "            smoothed_data    = convolve2d(heatmap_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_min_cis = convolve2d(min_cis_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            smoothed_max_cis = convolve2d(max_cis_data, kernel, mode='same', boundary='fill', fillvalue=0) \n",
    "\n",
    "            pos_data = np.where(smoothed_min_cis > 0.5, 1, 0)\n",
    "            neg_data = np.where(smoothed_max_cis < 0.5, 1, 0)\n",
    "\n",
    "            final_data = pos_data + neg_data\n",
    "\n",
    "            #smoothed_data = smoothed_data * final_data\n",
    "            # SHOW AND HIDE CBAR\n",
    "            cbar_bool = True\n",
    "            ax2 = sns.heatmap(smoothed_data, vmin = 0, vmax = 1, xticklabels = time_windows, yticklabels = time_windows, cmap = cmap,annot=False, square=True, cbar = True)\n",
    "            original_position = ax.get_position()\n",
    "            if count != 1:\n",
    "                cbar_bool = False\n",
    "                #sns.cbar = cbar_bool\n",
    "                cbar = ax.collections[0].colorbar\n",
    "                \n",
    "                cbar.remove()\n",
    "                ax.set_position(original_position)\n",
    "            # HIDE TICKS\n",
    "            show_every = 12\n",
    "            for i, label in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().xaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            for i, label in enumerate(plt.gca().yaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().yaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            # Plot your heatmap here, and set the x-axis ticks to the new time windows\n",
    "            #plt.xticks(range(len(new_time_windows)), new_time_windows)\n",
    "            plt.contour(final_data, colors='black', linestyles='dotted', linewidths=.5, alpha = 0.5)\n",
    "            #sns.set_palette(\"RdBu\")\n",
    "            count = count + 1\n",
    "            start_index = labels[0][l].find(\"Train\")\n",
    "            end_index = labels[0][l].find(\"Test\")\n",
    "            #stim = find_word_stim(title)\n",
    "            between_train_and_iteration = labels[0][l][start_index:end_index].strip()\n",
    "            new_title = f\"Average AUCs, {between_train_and_iteration}\"\n",
    "            \n",
    "            plt.gca().invert_yaxis()\n",
    "            # Show the plot\n",
    "            # PLOT DIAGNOAL\n",
    "            plt.plot([0, avg_scores.shape[2]], [0, avg_scores.shape[0]], color='black', linestyle='-', linewidth=.5, alpha = 0.3)\n",
    "            plt.grid(False)\n",
    "    \n",
    "    # Creates a new empty subplot emcompassing the whole figure (111)\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axis\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    # Adds the labels to this big new subplot\n",
    "    plt.xlabel('Test', fontsize=18)\n",
    "    plt.ylabel('Train', fontsize=18)\n",
    "\n",
    "    # Annotate the plot with subject information\n",
    "    sub_info = splits[0][0].subject\n",
    "    subtitle = f'Avg Subj. {splits[0][0].subject.subject} Round {splits[0][0].round_number} Lab {sub_info.lab}, Model {splits[0][0].model.__class__.__name__}, Average of {splits[0][0].average} Window length {window_size}, Step size {step_size}'\n",
    "\n",
    "# Add a square at the top right of the plot containing the subject information\n",
    "#plt.text(0.5, 1.1, info_text, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "# legs = ''\n",
    "    \n",
    "    plt.suptitle('\\n'+ new_title, fontsize=16,y=1.10)\n",
    "    plt.figtext(0.5, 0.95, subtitle, ha='center', va='center')\n",
    "    #filename = f'{models_name[i]}_{mods}_{window_size}_step_{step_size}_{formatted_datetime}_{num_splits}fold_{avg_num}avg'\n",
    "    plt.savefig( subtitle + new_title + 'subplots' + '.png', format='png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file d:\\ML_models\\IB_paper\\IB_eeg_data_BLINDED_phase1.pkl\n",
      "Loading file d:\\ML_models\\IB_paper\\IB_eeg_data_BLINDED_phase2.pkl\n",
      "Filtering Data based on fgroups\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Subject_Data at 0x16beca7d850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "##  DEFINING MODELS TO BE RUN\n",
    "is_cnn = False\n",
    "#models = [cnn]\n",
    "#models_name = ['CNN']\n",
    "\n",
    "haar = False\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "logreg = LogisticRegression(C=1,penalty=\"l2\",tol=0.01,solver=\"saga\",max_iter=10000)\n",
    "linreg_model = LinearRegression()\n",
    "\n",
    "# Create a Decision Tree stump as a base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoostClassifier with the Decision Tree stump as the base estimator\n",
    "adaBoost = AdaBoostClassifier(estimator=base_estimator, n_estimators=100)\n",
    "\n",
    "XGB_model = xgb.XGBClassifier(n_estimators=100, max_depth=1)\n",
    "LGB_model = lgb.LGBMClassifier(n_estimators=400, max_depth=20,num_leaves=20,n_jobs=-1,verbose=-1)\n",
    "forest = RandomForestClassifier(n_estimators=2000,criterion=\"entropy\",random_state=0,n_jobs=-1)\n",
    "knn = KNeighborsClassifier(n_neighbors=22, n_jobs=-1)\n",
    "svc_poly = SVC(kernel='poly',gamma='scale')\n",
    "svc_rbf = SVC(kernel='rbf',gamma='scale')\n",
    "svc_sigmoid = SVC(kernel='sigmoid',gamma='scale')\n",
    "svc = SVC(kernel='linear',gamma='scale')\n",
    "\n",
    "\n",
    "## DEFFINING THE CHOICES FOR THE MODELS\n",
    "models = [logreg]\n",
    "models_names = [model.__class__.__name__ for model in models]\n",
    "#models_name = ['LogReg']\n",
    "\n",
    "step_size = [5]\n",
    "window_sizes = [10]\n",
    "windows = [n for n in window_sizes]\n",
    "channels = [1,5,14]\n",
    "mods = 'raw'\n",
    "avgs = [8]\n",
    "percent = 70\n",
    "num_of_splits = 2\n",
    "\n",
    "#if file_name.find('deNoised'):\n",
    "mods = 'IB_TG_NoiseRemoval_RoundRobin'\n",
    "#if haar:\n",
    "#    mods = mods + 'haar'\n",
    "\n",
    "window_of_time = [500,1000]  # default -100 500\n",
    "win_size = np.shape(windows)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "lab = 'reed'\n",
    "subject = 'all'\n",
    "all_data = Subject_Data(subject, num_splits = 2, lab = lab, directory = 'n/a',new=1,folder_path = os.getcwd(), include_patterns=['_BLINDED_'], exclude_patterns=['NOTBLINDED','phase3'])\n",
    "all_data.filter_data('groups', 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Data based on fsubject_id\n",
      "[ 2  5  6  9 11 18 20 23 28 34 37 38 39 43 44 50 52 55 58 59 60 61 66 70\n",
      " 76 77 80 83 94 95 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "all_data.filter_data('subject_id', [1,48,51,72,91,93], include=False)\n",
    "print(np.unique(all_data.subject_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details open>\n",
       "    <summary><strong>General</strong></summary>\n",
       "    <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "        <tr>\n",
       "            <th>Measurement date</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <th>Experimenter</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <th>Participant</th>\n",
       "            \n",
       "            <td>Unknown</td>\n",
       "            \n",
       "        </tr>\n",
       "    </table>\n",
       "    </details>\n",
       "    <details open>\n",
       "        <summary><strong>Channels</strong></summary>\n",
       "        <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "            <tr>\n",
       "                <th>Digitized points</th>\n",
       "                \n",
       "                <td>62 points</td>\n",
       "                \n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>Good channels</th>\n",
       "                <td>59 EEG</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>Bad channels</th>\n",
       "                <td>None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>EOG channels</th>\n",
       "                <td>Not available</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th>ECG channels</th>\n",
       "                <td>Not available</td>\n",
       "            </tr>\n",
       "        </table>\n",
       "        </details>\n",
       "        <details open>\n",
       "            <summary><strong>Data</strong></summary>\n",
       "            <table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "                \n",
       "                <tr>\n",
       "                    <th>Sampling frequency</th>\n",
       "                    <td>500.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Highpass</th>\n",
       "                    <td>0.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                <tr>\n",
       "                    <th>Lowpass</th>\n",
       "                    <td>250.00 Hz</td>\n",
       "                </tr>\n",
       "                \n",
       "                \n",
       "                \n",
       "                \n",
       "            </table>\n",
       "            </details>"
      ],
      "text/plain": [
       "<Info | 8 non-empty values\n",
       " bads: []\n",
       " ch_names: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ...\n",
       " chs: 59 EEG\n",
       " custom_ref_applied: False\n",
       " dig: 62 items (3 Cardinal, 59 EEG)\n",
       " highpass: 0.0 Hz\n",
       " lowpass: 250.0 Hz\n",
       " meas_date: unspecified\n",
       " nchan: 59\n",
       " projs: []\n",
       " sfreq: 500.0 Hz\n",
       ">"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create MNE info\n",
    "sfreq = 500  # Sampling frequency\n",
    "montage = mne.channels.read_custom_montage('Reed_HM_59.loc')\n",
    "n_channels = len(montage.ch_names)\n",
    "ch_names = montage.ch_names\n",
    "ch_types = ['eeg'] * n_channels\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "info.set_montage(montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "\n",
    "# nums = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# def f(x):\n",
    "#     return x * x\n",
    "\n",
    "# def main():\n",
    "#     # Make sure the map and function are working\n",
    "#     print([val for val in map(f, nums)])\n",
    "\n",
    "#     # Test to make sure concurrent map is working\n",
    "#     with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#         print([val for val in executor.map(f, nums)])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running subject 2\n",
      "Filtering Data based on fsubject_id\n",
      "Subj Data size after filtering (1115, 64, 500)\n",
      "LogisticRegression\n",
      "8\n",
      "10\n",
      "Starting round number 1 out of 3\n",
      "Starting split number 1 out of 2\n",
      "Starting moving_window_subsample...\n",
      "Removing Noise from Dataset\n",
      "Train indices:  [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 138, 140, 141, 142, 143, 144, 145, 148, 149, 150, 151, 152, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 195, 196, 198, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 291, 292, 293, 294, 295, 298, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 336, 337, 338, 339, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 379, 380, 381, 382, 383, 385, 388, 389, 390, 391, 392, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 410, 411, 412, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 441, 442, 444, 445, 446, 449, 450, 451, 452, 455, 456, 457, 458, 459, 460, 461, 463, 465, 467, 468, 470, 471, 472, 473, 477, 478, 479, 480, 481, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 503, 504, 505, 509, 511, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 526, 527, 528, 529, 530, 532, 533, 535, 536, 537, 538, 539, 540, 541, 542, 544, 545, 547, 548, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 568, 571, 572, 573, 574, 575, 576, 578, 580, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 606, 611, 612, 613, 614, 617, 618, 619, 620, 621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 638, 639, 640, 641, 642, 643, 645, 646, 647, 649, 650, 651, 652, 653, 655, 656, 659, 663, 664, 665, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 681, 683, 685, 686, 687, 688, 689, 690, 691, 692, 693, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 707, 708, 709, 710, 712, 713, 714, 715, 716, 717, 719, 720, 721, 722, 723, 724, 725, 726, 727, 729, 730, 731, 732, 734, 735, 736, 737, 738, 739, 740, 743, 746, 747, 748, 749, 750, 751, 753, 755, 756, 759, 760, 761, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 774, 775, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 804, 805, 806, 807, 809, 810, 811, 815, 816, 817, 818, 819, 820, 821, 822, 824, 825, 826, 827, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 850, 851, 853, 854, 855, 856, 858, 859, 861, 862, 863, 865, 867, 868, 869, 870, 871, 872, 873, 874, 875, 877, 879, 880, 881, 882, 883, 884, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 912, 913, 914, 916, 917, 918, 919, 921, 922, 923, 924, 925, 927, 928, 929, 930, 931, 932, 935, 936, 937, 938, 940, 942, 944, 945, 946, 947, 948, 949, 950, 951, 953, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 974, 975, 976, 978, 979, 981, 984, 985, 986, 988, 989, 990, 992, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1014, 1015, 1016, 1017, 1018, 1019, 1021, 1024, 1025, 1028, 1029, 1030, 1032, 1034, 1035, 1036, 1037, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1062, 1063, 1064, 1065, 1066, 1067, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1077, 1080, 1081, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1094, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\n",
      "Test indices:  [5, 17, 20, 22, 36, 39, 48, 49, 52, 55, 63, 64, 67, 75, 83, 102, 103, 109, 116, 133, 135, 137, 139, 146, 147, 153, 154, 161, 171, 172, 179, 189, 194, 197, 199, 202, 215, 216, 217, 222, 223, 241, 243, 244, 256, 258, 275, 287, 290, 296, 297, 299, 303, 318, 320, 332, 335, 340, 344, 353, 354, 375, 377, 378, 384, 386, 387, 393, 404, 408, 409, 413, 414, 425, 435, 439, 440, 443, 447, 448, 453, 454, 462, 464, 466, 469, 474, 475, 476, 482, 486, 501, 506, 507, 508, 510, 512, 513, 525, 531, 534, 543, 546, 549, 567, 569, 570, 577, 579, 581, 583, 595, 604, 605, 607, 608, 609, 610, 615, 616, 622, 637, 644, 648, 654, 657, 658, 660, 661, 662, 666, 680, 682, 684, 694, 695, 706, 711, 718, 728, 733, 741, 742, 744, 745, 752, 754, 757, 758, 762, 773, 776, 788, 803, 808, 812, 813, 814, 823, 828, 829, 830, 849, 852, 857, 860, 864, 866, 876, 878, 885, 886, 908, 915, 920, 926, 933, 934, 939, 941, 943, 952, 954, 973, 977, 980, 982, 983, 987, 991, 993, 994, 1005, 1013, 1020, 1022, 1023, 1026, 1027, 1031, 1033, 1038, 1054, 1061, 1068, 1076, 1078, 1079, 1082, 1091, 1095, 1096, 1105, 1106]\n",
      "Train Noise Shape:  (291, 64, 500)\n",
      "Train Noise avg Shape:  (64, 500)\n",
      "training data shape:  (901, 64, 500)\n",
      "training data shape:  (901, 64, 500)\n",
      "Subtraction Completed\n",
      "Train data (299, 64, 500)\n",
      "Submitting 98 tasks to the executor...\n",
      "Process 10852 working on start_time: 0\n",
      "Process 10852 working on start_time: 5\n",
      "Process 10852 working on start_time: 10\n",
      "Process 10852 working on start_time: 15\n",
      "Process 10852 working on start_time: 20\n",
      "Process 10852 working on start_time: 25\n",
      "Process 10852 working on start_time: 30\n",
      "Process 10852 working on start_time: 35\n",
      "Process 10852 working on start_time: 40\n",
      "Process 10852 working on start_time: 45\n",
      "Process 10852 working on start_time: 50\n",
      "Process 10852 working on start_time: 55\n",
      "Process 10852 working on start_time: 60\n",
      "Process 10852 working on start_time: 65\n",
      "Process 10852 working on start_time: 70\n",
      "Process 10852 working on start_time: 75\n",
      "Process 10852 working on start_time: 80\n",
      "Process 10852 working on start_time: 85\n",
      "Process 10852 working on start_time: 90\n",
      "Process 10852 working on start_time: 95\n",
      "Process 10852 working on start_time: 100\n",
      "Process 10852 working on start_time: 105\n",
      "Process 10852 working on start_time: 110\n",
      "Process 10852 working on start_time: 115\n",
      "Process 10852 working on start_time: 120\n",
      "Process 10852 working on start_time: 125\n",
      "Process 10852 working on start_time: 130\n",
      "Process 10852 working on start_time: 135\n",
      "Process 10852 working on start_time: 140\n",
      "Process 10852 working on start_time: 145\n",
      "Process 10852 working on start_time: 150\n",
      "Process 10852 working on start_time: 155\n",
      "Process 10852 working on start_time: 160\n",
      "Process 10852 working on start_time: 165\n",
      "Process 10852 working on start_time: 170\n",
      "Process 10852 working on start_time: 175\n",
      "Process 10852 working on start_time: 180\n",
      "Process 10852 working on start_time: 185\n",
      "Process 10852 working on start_time: 190\n",
      "Process 10852 working on start_time: 195\n",
      "Process 10852 working on start_time: 200\n",
      "Process 10852 working on start_time: 205\n",
      "Process 10852 working on start_time: 210\n",
      "Process 10852 working on start_time: 215\n",
      "Process 10852 working on start_time: 220\n",
      "Process 10852 working on start_time: 225\n",
      "Process 10852 working on start_time: 230\n",
      "Process 10852 working on start_time: 235\n",
      "Process 10852 working on start_time: 240\n",
      "Process 10852 working on start_time: 245\n",
      "Process 10852 working on start_time: 250\n",
      "Process 10852 working on start_time: 255\n",
      "Process 10852 working on start_time: 260\n",
      "Process 10852 working on start_time: 265\n",
      "Process 10852 working on start_time: 270\n",
      "Process 10852 working on start_time: 275\n",
      "Process 10852 working on start_time: 280\n",
      "Process 10852 working on start_time: 285\n",
      "Process 10852 working on start_time: 290\n",
      "Process 10852 working on start_time: 295\n",
      "Process 10852 working on start_time: 300\n",
      "Process 10852 working on start_time: 305\n",
      "Process 10852 working on start_time: 310\n",
      "Process 10852 working on start_time: 315\n",
      "Process 10852 working on start_time: 320\n",
      "Process 10852 working on start_time: 325\n",
      "Process 10852 working on start_time: 330\n",
      "Process 10852 working on start_time: 335\n",
      "Process 10852 working on start_time: 340\n",
      "Process 10852 working on start_time: 345\n",
      "Process 10852 working on start_time: 350\n",
      "Process 10852 working on start_time: 355\n",
      "Process 10852 working on start_time: 360\n",
      "Process 10852 working on start_time: 365\n",
      "Process 10852 working on start_time: 370\n",
      "Process 10852 working on start_time: 375\n",
      "Process 10852 working on start_time: 380\n",
      "Process 10852 working on start_time: 385\n",
      "Process 10852 working on start_time: 390\n",
      "Process 10852 working on start_time: 395\n",
      "Process 10852 working on start_time: 400\n",
      "Process 10852 working on start_time: 405\n",
      "Process 10852 working on start_time: 410\n",
      "Process 10852 working on start_time: 415\n",
      "Process 10852 working on start_time: 420\n",
      "Process 10852 working on start_time: 425\n",
      "Process 10852 working on start_time: 430\n",
      "Process 10852 working on start_time: 435\n",
      "Process 10852 working on start_time: 440\n",
      "Process 10852 working on start_time: 445\n",
      "Process 10852 working on start_time: 450\n",
      "Process 10852 working on start_time: 455\n",
      "Process 10852 working on start_time: 460\n",
      "Process 10852 working on start_time: 465\n",
      "Process 10852 working on start_time: 470\n",
      "Process 10852 working on start_time: 475\n",
      "Process 10852 working on start_time: 480\n",
      "Process 10852 working on start_time: 485\n",
      "Saved Run 0 Time: 83.43046649999997\n",
      "Starting split number 2 out of 2\n",
      "Starting moving_window_subsample...\n",
      "Removing Noise from Dataset\n",
      "Train indices:  [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 83, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 102, 103, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 131, 132, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 146, 147, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 164, 165, 166, 168, 169, 170, 171, 172, 174, 175, 177, 178, 179, 180, 181, 182, 183, 185, 186, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 329, 330, 331, 332, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 419, 421, 423, 424, 425, 426, 430, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 464, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 480, 481, 482, 483, 485, 486, 487, 488, 490, 491, 492, 493, 494, 496, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 510, 511, 512, 513, 515, 516, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 533, 534, 535, 536, 537, 538, 540, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 567, 568, 569, 570, 571, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 587, 588, 589, 590, 591, 593, 594, 595, 599, 602, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 622, 623, 624, 625, 627, 628, 629, 631, 632, 633, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 656, 657, 658, 659, 660, 661, 662, 663, 665, 666, 667, 668, 669, 670, 671, 672, 675, 676, 678, 679, 680, 681, 682, 683, 684, 689, 690, 691, 694, 695, 697, 698, 700, 702, 703, 704, 705, 706, 708, 709, 711, 712, 713, 714, 715, 716, 718, 719, 720, 721, 722, 723, 724, 726, 727, 728, 730, 733, 734, 735, 736, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 750, 751, 752, 753, 754, 756, 757, 758, 759, 760, 762, 763, 764, 765, 766, 767, 768, 769, 770, 773, 774, 775, 776, 777, 778, 779, 780, 785, 786, 787, 788, 789, 790, 793, 794, 797, 798, 800, 802, 803, 804, 805, 806, 807, 808, 809, 811, 812, 813, 814, 815, 817, 818, 819, 821, 823, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 839, 840, 841, 842, 843, 845, 846, 848, 849, 852, 855, 857, 858, 859, 860, 861, 862, 864, 865, 866, 867, 868, 869, 870, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 884, 885, 886, 887, 888, 889, 890, 891, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 913, 914, 915, 917, 918, 919, 920, 921, 923, 924, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 938, 939, 940, 941, 943, 944, 945, 946, 947, 948, 949, 951, 952, 953, 954, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 967, 968, 969, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1001, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1061, 1062, 1063, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1094, 1095, 1096, 1097, 1098, 1100, 1101, 1102, 1103, 1105, 1106, 1109, 1111, 1112, 1113, 1114]\n",
      "Test indices:  [2, 3, 4, 16, 24, 25, 34, 43, 61, 77, 81, 82, 84, 86, 87, 97, 101, 104, 108, 112, 127, 130, 138, 140, 145, 148, 151, 160, 162, 167, 173, 176, 184, 187, 195, 209, 228, 229, 234, 240, 248, 254, 257, 262, 266, 284, 288, 292, 305, 308, 314, 322, 327, 328, 333, 334, 341, 347, 360, 372, 374, 385, 389, 392, 411, 418, 420, 422, 427, 428, 429, 431, 446, 449, 459, 463, 465, 470, 479, 484, 489, 495, 497, 498, 509, 514, 517, 529, 539, 541, 566, 572, 575, 584, 585, 586, 592, 596, 597, 598, 600, 601, 603, 606, 621, 626, 630, 634, 635, 636, 655, 664, 673, 674, 677, 685, 686, 687, 688, 692, 693, 696, 699, 701, 707, 710, 717, 725, 729, 731, 732, 737, 738, 749, 755, 761, 771, 772, 781, 782, 783, 784, 791, 792, 795, 796, 799, 801, 810, 816, 820, 822, 824, 831, 837, 838, 844, 847, 850, 851, 853, 854, 856, 863, 871, 882, 883, 892, 893, 911, 912, 916, 922, 925, 931, 937, 942, 950, 955, 966, 970, 984, 1000, 1002, 1014, 1029, 1050, 1059, 1060, 1064, 1072, 1073, 1074, 1085, 1093, 1099, 1104, 1107, 1108, 1110]\n",
      "Train Noise Shape:  (316, 64, 500)\n",
      "Train Noise avg Shape:  (64, 500)\n",
      "training data shape:  (915, 64, 500)\n",
      "training data shape:  (915, 64, 500)\n",
      "Subtraction Completed\n",
      "Train data (293, 64, 500)\n",
      "Submitting 98 tasks to the executor...\n",
      "Process 10852 working on start_time: 0\n",
      "Process 10852 working on start_time: 5\n",
      "Process 10852 working on start_time: 10\n",
      "Process 10852 working on start_time: 15\n",
      "Process 10852 working on start_time: 20\n",
      "Process 10852 working on start_time: 25\n",
      "Process 10852 working on start_time: 30\n",
      "Process 10852 working on start_time: 35\n",
      "Process 10852 working on start_time: 40\n",
      "Process 10852 working on start_time: 45\n",
      "Process 10852 working on start_time: 50\n",
      "Process 10852 working on start_time: 55\n",
      "Process 10852 working on start_time: 60\n",
      "Process 10852 working on start_time: 65\n",
      "Process 10852 working on start_time: 70\n",
      "Process 10852 working on start_time: 75\n",
      "Process 10852 working on start_time: 80\n",
      "Process 10852 working on start_time: 85\n",
      "Process 10852 working on start_time: 90\n",
      "Process 10852 working on start_time: 95\n",
      "Process 10852 working on start_time: 100\n",
      "Process 10852 working on start_time: 105\n",
      "Process 10852 working on start_time: 110\n",
      "Process 10852 working on start_time: 115\n",
      "Process 10852 working on start_time: 120\n",
      "Process 10852 working on start_time: 125\n",
      "Process 10852 working on start_time: 130\n",
      "Process 10852 working on start_time: 135\n",
      "Process 10852 working on start_time: 140\n",
      "Process 10852 working on start_time: 145\n",
      "Process 10852 working on start_time: 150\n",
      "Process 10852 working on start_time: 155\n",
      "Process 10852 working on start_time: 160\n",
      "Process 10852 working on start_time: 165\n",
      "Process 10852 working on start_time: 170\n",
      "Process 10852 working on start_time: 175\n",
      "Process 10852 working on start_time: 180\n",
      "Process 10852 working on start_time: 185\n",
      "Process 10852 working on start_time: 190\n",
      "Process 10852 working on start_time: 195\n",
      "Process 10852 working on start_time: 200\n",
      "Process 10852 working on start_time: 205\n",
      "Process 10852 working on start_time: 210\n",
      "Process 10852 working on start_time: 215\n",
      "Process 10852 working on start_time: 220\n",
      "Process 10852 working on start_time: 225\n",
      "Process 10852 working on start_time: 230\n",
      "Process 10852 working on start_time: 235\n",
      "Process 10852 working on start_time: 240\n",
      "Process 10852 working on start_time: 245\n",
      "Process 10852 working on start_time: 250\n",
      "Process 10852 working on start_time: 255\n",
      "Process 10852 working on start_time: 260\n",
      "Process 10852 working on start_time: 265\n",
      "Process 10852 working on start_time: 270\n",
      "Process 10852 working on start_time: 275\n",
      "Process 10852 working on start_time: 280\n",
      "Process 10852 working on start_time: 285\n",
      "Process 10852 working on start_time: 290\n",
      "Process 10852 working on start_time: 295\n",
      "Process 10852 working on start_time: 300\n",
      "Process 10852 working on start_time: 305\n",
      "Process 10852 working on start_time: 310\n",
      "Process 10852 working on start_time: 315\n",
      "Process 10852 working on start_time: 320\n",
      "Process 10852 working on start_time: 325\n",
      "Process 10852 working on start_time: 330\n",
      "Process 10852 working on start_time: 335\n",
      "Process 10852 working on start_time: 340\n",
      "Process 10852 working on start_time: 345\n",
      "Process 10852 working on start_time: 350\n",
      "Process 10852 working on start_time: 355\n",
      "Process 10852 working on start_time: 360\n",
      "Process 10852 working on start_time: 365\n",
      "Process 10852 working on start_time: 370\n",
      "Process 10852 working on start_time: 375\n",
      "Process 10852 working on start_time: 380\n",
      "Process 10852 working on start_time: 385\n",
      "Process 10852 working on start_time: 390\n",
      "Process 10852 working on start_time: 395\n",
      "Process 10852 working on start_time: 400\n",
      "Process 10852 working on start_time: 405\n",
      "Process 10852 working on start_time: 410\n",
      "Process 10852 working on start_time: 415\n",
      "Process 10852 working on start_time: 420\n",
      "Process 10852 working on start_time: 425\n",
      "Process 10852 working on start_time: 430\n",
      "Process 10852 working on start_time: 435\n",
      "Process 10852 working on start_time: 440\n",
      "Process 10852 working on start_time: 445\n",
      "Process 10852 working on start_time: 450\n",
      "Process 10852 working on start_time: 455\n",
      "Process 10852 working on start_time: 460\n",
      "Process 10852 working on start_time: 465\n",
      "Process 10852 working on start_time: 470\n",
      "Process 10852 working on start_time: 475\n",
      "Process 10852 working on start_time: 480\n",
      "Process 10852 working on start_time: 485\n",
      "Saved Run 1 Time: 82.35329390000015\n",
      "Starting round number 2 out of 3\n",
      "Starting split number 1 out of 2\n",
      "Starting moving_window_subsample...\n",
      "Removing Noise from Dataset\n",
      "Train indices:  [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 138, 140, 141, 142, 143, 144, 145, 148, 149, 150, 151, 152, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 195, 196, 198, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 291, 292, 293, 294, 295, 298, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 336, 337, 338, 339, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 379, 380, 381, 382, 383, 385, 388, 389, 390, 391, 392, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 410, 411, 412, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 441, 442, 444, 445, 446, 449, 450, 451, 452, 455, 456, 457, 458, 459, 460, 461, 463, 465, 467, 468, 470, 471, 472, 473, 477, 478, 479, 480, 481, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 503, 504, 505, 509, 511, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 526, 527, 528, 529, 530, 532, 533, 535, 536, 537, 538, 539, 540, 541, 542, 544, 545, 547, 548, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 568, 571, 572, 573, 574, 575, 576, 578, 580, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 606, 611, 612, 613, 614, 617, 618, 619, 620, 621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 638, 639, 640, 641, 642, 643, 645, 646, 647, 649, 650, 651, 652, 653, 655, 656, 659, 663, 664, 665, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 681, 683, 685, 686, 687, 688, 689, 690, 691, 692, 693, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 707, 708, 709, 710, 712, 713, 714, 715, 716, 717, 719, 720, 721, 722, 723, 724, 725, 726, 727, 729, 730, 731, 732, 734, 735, 736, 737, 738, 739, 740, 743, 746, 747, 748, 749, 750, 751, 753, 755, 756, 759, 760, 761, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 774, 775, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 804, 805, 806, 807, 809, 810, 811, 815, 816, 817, 818, 819, 820, 821, 822, 824, 825, 826, 827, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 850, 851, 853, 854, 855, 856, 858, 859, 861, 862, 863, 865, 867, 868, 869, 870, 871, 872, 873, 874, 875, 877, 879, 880, 881, 882, 883, 884, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 912, 913, 914, 916, 917, 918, 919, 921, 922, 923, 924, 925, 927, 928, 929, 930, 931, 932, 935, 936, 937, 938, 940, 942, 944, 945, 946, 947, 948, 949, 950, 951, 953, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 974, 975, 976, 978, 979, 981, 984, 985, 986, 988, 989, 990, 992, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1014, 1015, 1016, 1017, 1018, 1019, 1021, 1024, 1025, 1028, 1029, 1030, 1032, 1034, 1035, 1036, 1037, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1062, 1063, 1064, 1065, 1066, 1067, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1077, 1080, 1081, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1094, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\n",
      "Test indices:  [5, 17, 20, 22, 36, 39, 48, 49, 52, 55, 63, 64, 67, 75, 83, 102, 103, 109, 116, 133, 135, 137, 139, 146, 147, 153, 154, 161, 171, 172, 179, 189, 194, 197, 199, 202, 215, 216, 217, 222, 223, 241, 243, 244, 256, 258, 275, 287, 290, 296, 297, 299, 303, 318, 320, 332, 335, 340, 344, 353, 354, 375, 377, 378, 384, 386, 387, 393, 404, 408, 409, 413, 414, 425, 435, 439, 440, 443, 447, 448, 453, 454, 462, 464, 466, 469, 474, 475, 476, 482, 486, 501, 506, 507, 508, 510, 512, 513, 525, 531, 534, 543, 546, 549, 567, 569, 570, 577, 579, 581, 583, 595, 604, 605, 607, 608, 609, 610, 615, 616, 622, 637, 644, 648, 654, 657, 658, 660, 661, 662, 666, 680, 682, 684, 694, 695, 706, 711, 718, 728, 733, 741, 742, 744, 745, 752, 754, 757, 758, 762, 773, 776, 788, 803, 808, 812, 813, 814, 823, 828, 829, 830, 849, 852, 857, 860, 864, 866, 876, 878, 885, 886, 908, 915, 920, 926, 933, 934, 939, 941, 943, 952, 954, 973, 977, 980, 982, 983, 987, 991, 993, 994, 1005, 1013, 1020, 1022, 1023, 1026, 1027, 1031, 1033, 1038, 1054, 1061, 1068, 1076, 1078, 1079, 1082, 1091, 1095, 1096, 1105, 1106]\n",
      "Train Noise Shape:  (291, 64, 500)\n",
      "Train Noise avg Shape:  (64, 500)\n",
      "training data shape:  (901, 64, 500)\n",
      "training data shape:  (901, 64, 500)\n",
      "Subtraction Completed\n",
      "Train data (311, 64, 500)\n",
      "Submitting 98 tasks to the executor...\n",
      "Process 10852 working on start_time: 0\n",
      "Process 10852 working on start_time: 5\n",
      "Process 10852 working on start_time: 10\n",
      "Process 10852 working on start_time: 15\n",
      "Process 10852 working on start_time: 20\n",
      "Process 10852 working on start_time: 25\n",
      "Process 10852 working on start_time: 30\n",
      "Process 10852 working on start_time: 35\n",
      "Process 10852 working on start_time: 40\n",
      "Process 10852 working on start_time: 45\n",
      "Process 10852 working on start_time: 50\n",
      "Process 10852 working on start_time: 55\n",
      "Process 10852 working on start_time: 60\n",
      "Process 10852 working on start_time: 65\n",
      "Process 10852 working on start_time: 70\n",
      "Process 10852 working on start_time: 75\n",
      "Process 10852 working on start_time: 80\n",
      "Process 10852 working on start_time: 85\n",
      "Process 10852 working on start_time: 90\n",
      "Process 10852 working on start_time: 95\n",
      "Process 10852 working on start_time: 100\n",
      "Process 10852 working on start_time: 105\n",
      "Process 10852 working on start_time: 110\n",
      "Process 10852 working on start_time: 115\n",
      "Process 10852 working on start_time: 120\n",
      "Process 10852 working on start_time: 125\n",
      "Process 10852 working on start_time: 130\n",
      "Process 10852 working on start_time: 135\n",
      "Process 10852 working on start_time: 140\n",
      "Process 10852 working on start_time: 145\n",
      "Process 10852 working on start_time: 150\n",
      "Process 10852 working on start_time: 155\n",
      "Process 10852 working on start_time: 160\n",
      "Process 10852 working on start_time: 165\n",
      "Process 10852 working on start_time: 170\n",
      "Process 10852 working on start_time: 175\n",
      "Process 10852 working on start_time: 180\n",
      "Process 10852 working on start_time: 185\n",
      "Process 10852 working on start_time: 190\n",
      "Process 10852 working on start_time: 195\n",
      "Process 10852 working on start_time: 200\n",
      "Process 10852 working on start_time: 205\n",
      "Process 10852 working on start_time: 210\n",
      "Process 10852 working on start_time: 215\n",
      "Process 10852 working on start_time: 220\n",
      "Process 10852 working on start_time: 225\n",
      "Process 10852 working on start_time: 230\n",
      "Process 10852 working on start_time: 235\n",
      "Process 10852 working on start_time: 240\n",
      "Process 10852 working on start_time: 245\n",
      "Process 10852 working on start_time: 250\n",
      "Process 10852 working on start_time: 255\n",
      "Process 10852 working on start_time: 260\n",
      "Process 10852 working on start_time: 265\n",
      "Process 10852 working on start_time: 270\n",
      "Process 10852 working on start_time: 275\n",
      "Process 10852 working on start_time: 280\n",
      "Process 10852 working on start_time: 285\n",
      "Process 10852 working on start_time: 290\n",
      "Process 10852 working on start_time: 295\n",
      "Process 10852 working on start_time: 300\n",
      "Process 10852 working on start_time: 305\n",
      "Process 10852 working on start_time: 310\n",
      "Process 10852 working on start_time: 315\n",
      "Process 10852 working on start_time: 320\n",
      "Process 10852 working on start_time: 325\n",
      "Process 10852 working on start_time: 330\n",
      "Process 10852 working on start_time: 335\n",
      "Process 10852 working on start_time: 340\n",
      "Process 10852 working on start_time: 345\n",
      "Process 10852 working on start_time: 350\n",
      "Process 10852 working on start_time: 355\n",
      "Process 10852 working on start_time: 360\n",
      "Process 10852 working on start_time: 365\n",
      "Process 10852 working on start_time: 370\n",
      "Process 10852 working on start_time: 375\n",
      "Process 10852 working on start_time: 380\n",
      "Process 10852 working on start_time: 385\n",
      "Process 10852 working on start_time: 390\n",
      "Process 10852 working on start_time: 395\n",
      "Process 10852 working on start_time: 400\n",
      "Process 10852 working on start_time: 405\n",
      "Process 10852 working on start_time: 410\n",
      "Process 10852 working on start_time: 415\n",
      "Process 10852 working on start_time: 420\n",
      "Process 10852 working on start_time: 425\n",
      "Process 10852 working on start_time: 430\n",
      "Process 10852 working on start_time: 435\n",
      "Process 10852 working on start_time: 440\n",
      "Process 10852 working on start_time: 445\n",
      "Process 10852 working on start_time: 450\n",
      "Process 10852 working on start_time: 455\n",
      "Process 10852 working on start_time: 460\n",
      "Process 10852 working on start_time: 465\n",
      "Process 10852 working on start_time: 470\n",
      "Process 10852 working on start_time: 475\n",
      "Process 10852 working on start_time: 480\n",
      "Process 10852 working on start_time: 485\n",
      "Saved Run 0 Time: 82.69644609999978\n",
      "Starting split number 2 out of 2\n",
      "Starting moving_window_subsample...\n",
      "Removing Noise from Dataset\n",
      "Train indices:  [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 83, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 102, 103, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 131, 132, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 146, 147, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 164, 165, 166, 168, 169, 170, 171, 172, 174, 175, 177, 178, 179, 180, 181, 182, 183, 185, 186, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 329, 330, 331, 332, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 419, 421, 423, 424, 425, 426, 430, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 464, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 480, 481, 482, 483, 485, 486, 487, 488, 490, 491, 492, 493, 494, 496, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 510, 511, 512, 513, 515, 516, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 533, 534, 535, 536, 537, 538, 540, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 567, 568, 569, 570, 571, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 587, 588, 589, 590, 591, 593, 594, 595, 599, 602, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 622, 623, 624, 625, 627, 628, 629, 631, 632, 633, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 656, 657, 658, 659, 660, 661, 662, 663, 665, 666, 667, 668, 669, 670, 671, 672, 675, 676, 678, 679, 680, 681, 682, 683, 684, 689, 690, 691, 694, 695, 697, 698, 700, 702, 703, 704, 705, 706, 708, 709, 711, 712, 713, 714, 715, 716, 718, 719, 720, 721, 722, 723, 724, 726, 727, 728, 730, 733, 734, 735, 736, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 750, 751, 752, 753, 754, 756, 757, 758, 759, 760, 762, 763, 764, 765, 766, 767, 768, 769, 770, 773, 774, 775, 776, 777, 778, 779, 780, 785, 786, 787, 788, 789, 790, 793, 794, 797, 798, 800, 802, 803, 804, 805, 806, 807, 808, 809, 811, 812, 813, 814, 815, 817, 818, 819, 821, 823, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 839, 840, 841, 842, 843, 845, 846, 848, 849, 852, 855, 857, 858, 859, 860, 861, 862, 864, 865, 866, 867, 868, 869, 870, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 884, 885, 886, 887, 888, 889, 890, 891, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 913, 914, 915, 917, 918, 919, 920, 921, 923, 924, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 938, 939, 940, 941, 943, 944, 945, 946, 947, 948, 949, 951, 952, 953, 954, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 967, 968, 969, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1001, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1061, 1062, 1063, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1094, 1095, 1096, 1097, 1098, 1100, 1101, 1102, 1103, 1105, 1106, 1109, 1111, 1112, 1113, 1114]\n",
      "Test indices:  [2, 3, 4, 16, 24, 25, 34, 43, 61, 77, 81, 82, 84, 86, 87, 97, 101, 104, 108, 112, 127, 130, 138, 140, 145, 148, 151, 160, 162, 167, 173, 176, 184, 187, 195, 209, 228, 229, 234, 240, 248, 254, 257, 262, 266, 284, 288, 292, 305, 308, 314, 322, 327, 328, 333, 334, 341, 347, 360, 372, 374, 385, 389, 392, 411, 418, 420, 422, 427, 428, 429, 431, 446, 449, 459, 463, 465, 470, 479, 484, 489, 495, 497, 498, 509, 514, 517, 529, 539, 541, 566, 572, 575, 584, 585, 586, 592, 596, 597, 598, 600, 601, 603, 606, 621, 626, 630, 634, 635, 636, 655, 664, 673, 674, 677, 685, 686, 687, 688, 692, 693, 696, 699, 701, 707, 710, 717, 725, 729, 731, 732, 737, 738, 749, 755, 761, 771, 772, 781, 782, 783, 784, 791, 792, 795, 796, 799, 801, 810, 816, 820, 822, 824, 831, 837, 838, 844, 847, 850, 851, 853, 854, 856, 863, 871, 882, 883, 892, 893, 911, 912, 916, 922, 925, 931, 937, 942, 950, 955, 966, 970, 984, 1000, 1002, 1014, 1029, 1050, 1059, 1060, 1064, 1072, 1073, 1074, 1085, 1093, 1099, 1104, 1107, 1108, 1110]\n",
      "Train Noise Shape:  (316, 64, 500)\n",
      "Train Noise avg Shape:  (64, 500)\n",
      "training data shape:  (915, 64, 500)\n",
      "training data shape:  (915, 64, 500)\n",
      "Subtraction Completed\n",
      "Train data (306, 64, 500)\n",
      "Submitting 98 tasks to the executor...\n",
      "Process 10852 working on start_time: 0\n",
      "Process 10852 working on start_time: 5\n",
      "Process 10852 working on start_time: 10\n",
      "Process 10852 working on start_time: 15\n",
      "Process 10852 working on start_time: 20\n",
      "Process 10852 working on start_time: 25\n",
      "Process 10852 working on start_time: 30\n",
      "Process 10852 working on start_time: 35\n",
      "Process 10852 working on start_time: 40\n",
      "Process 10852 working on start_time: 45\n",
      "Process 10852 working on start_time: 50\n",
      "Process 10852 working on start_time: 55\n",
      "Process 10852 working on start_time: 60\n",
      "Process 10852 working on start_time: 65\n",
      "Process 10852 working on start_time: 70\n",
      "Process 10852 working on start_time: 75\n",
      "Process 10852 working on start_time: 80\n",
      "Process 10852 working on start_time: 85\n",
      "Process 10852 working on start_time: 90\n",
      "Process 10852 working on start_time: 95\n",
      "Process 10852 working on start_time: 100\n",
      "Process 10852 working on start_time: 105\n",
      "Process 10852 working on start_time: 110\n",
      "Process 10852 working on start_time: 115\n",
      "Process 10852 working on start_time: 120\n",
      "Process 10852 working on start_time: 125\n",
      "Process 10852 working on start_time: 130\n",
      "Process 10852 working on start_time: 135\n",
      "Process 10852 working on start_time: 140\n",
      "Process 10852 working on start_time: 145\n",
      "Process 10852 working on start_time: 150\n",
      "Process 10852 working on start_time: 155\n",
      "Process 10852 working on start_time: 160\n",
      "Process 10852 working on start_time: 165\n",
      "Process 10852 working on start_time: 170\n",
      "Process 10852 working on start_time: 175\n",
      "Process 10852 working on start_time: 180\n",
      "Process 10852 working on start_time: 185\n",
      "Process 10852 working on start_time: 190\n",
      "Process 10852 working on start_time: 195\n",
      "Process 10852 working on start_time: 200\n",
      "Process 10852 working on start_time: 205\n",
      "Process 10852 working on start_time: 210\n",
      "Process 10852 working on start_time: 215\n",
      "Process 10852 working on start_time: 220\n",
      "Process 10852 working on start_time: 225\n",
      "Process 10852 working on start_time: 230\n",
      "Process 10852 working on start_time: 235\n",
      "Process 10852 working on start_time: 240\n",
      "Process 10852 working on start_time: 245\n",
      "Process 10852 working on start_time: 250\n",
      "Process 10852 working on start_time: 255\n",
      "Process 10852 working on start_time: 260\n",
      "Process 10852 working on start_time: 265\n",
      "Process 10852 working on start_time: 270\n",
      "Process 10852 working on start_time: 275\n",
      "Process 10852 working on start_time: 280\n",
      "Process 10852 working on start_time: 285\n",
      "Process 10852 working on start_time: 290\n",
      "Process 10852 working on start_time: 295\n",
      "Process 10852 working on start_time: 300Process 10852 working on start_time: 305\n",
      "\n",
      "Process 10852 working on start_time: 310\n",
      "Process 10852 working on start_time: 315\n",
      "Process 10852 working on start_time: 320\n",
      "Process 10852 working on start_time: 325\n",
      "Process 10852 working on start_time: 330\n",
      "Process 10852 working on start_time: 335\n",
      "Process 10852 working on start_time: 340\n",
      "Process 10852 working on start_time: 345\n",
      "Process 10852 working on start_time: 350\n",
      "Process 10852 working on start_time: 355\n",
      "Process 10852 working on start_time: 360\n",
      "Process 10852 working on start_time: 365\n",
      "Process 10852 working on start_time: 370\n",
      "Process 10852 working on start_time: 375\n",
      "Process 10852 working on start_time: 380\n",
      "Process 10852 working on start_time: 385\n",
      "Process 10852 working on start_time: 390\n",
      "Process 10852 working on start_time: 395\n",
      "Process 10852 working on start_time: 400\n",
      "Process 10852 working on start_time: 405\n",
      "Process 10852 working on start_time: 410\n",
      "Process 10852 working on start_time: 415\n",
      "Process 10852 working on start_time: 420\n",
      "Process 10852 working on start_time: 425\n",
      "Process 10852 working on start_time: 430\n",
      "Process 10852 working on start_time: 435\n",
      "Process 10852 working on start_time: 440\n",
      "Process 10852 working on start_time: 445\n",
      "Process 10852 working on start_time: 450\n",
      "Process 10852 working on start_time: 455\n",
      "Process 10852 working on start_time: 460\n",
      "Process 10852 working on start_time: 465\n",
      "Process 10852 working on start_time: 470\n",
      "Process 10852 working on start_time: 475\n",
      "Process 10852 working on start_time: 480\n",
      "Process 10852 working on start_time: 485\n",
      "Saved Run 1 Time: 82.60829019999983\n",
      "Starting round number 3 out of 3\n",
      "Starting split number 1 out of 2\n",
      "Starting moving_window_subsample...\n",
      "Removing Noise from Dataset\n",
      "Train indices:  [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 138, 140, 141, 142, 143, 144, 145, 148, 149, 150, 151, 152, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 195, 196, 198, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 291, 292, 293, 294, 295, 298, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 336, 337, 338, 339, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 352, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 379, 380, 381, 382, 383, 385, 388, 389, 390, 391, 392, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 406, 407, 410, 411, 412, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 441, 442, 444, 445, 446, 449, 450, 451, 452, 455, 456, 457, 458, 459, 460, 461, 463, 465, 467, 468, 470, 471, 472, 473, 477, 478, 479, 480, 481, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 503, 504, 505, 509, 511, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 526, 527, 528, 529, 530, 532, 533, 535, 536, 537, 538, 539, 540, 541, 542, 544, 545, 547, 548, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 568, 571, 572, 573, 574, 575, 576, 578, 580, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 606, 611, 612, 613, 614, 617, 618, 619, 620, 621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 638, 639, 640, 641, 642, 643, 645, 646, 647, 649, 650, 651, 652, 653, 655, 656, 659, 663, 664, 665, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 681, 683, 685, 686, 687, 688, 689, 690, 691, 692, 693, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 707, 708, 709, 710, 712, 713, 714, 715, 716, 717, 719, 720, 721, 722, 723, 724, 725, 726, 727, 729, 730, 731, 732, 734, 735, 736, 737, 738, 739, 740, 743, 746, 747, 748, 749, 750, 751, 753, 755, 756, 759, 760, 761, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 774, 775, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 804, 805, 806, 807, 809, 810, 811, 815, 816, 817, 818, 819, 820, 821, 822, 824, 825, 826, 827, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 850, 851, 853, 854, 855, 856, 858, 859, 861, 862, 863, 865, 867, 868, 869, 870, 871, 872, 873, 874, 875, 877, 879, 880, 881, 882, 883, 884, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 912, 913, 914, 916, 917, 918, 919, 921, 922, 923, 924, 925, 927, 928, 929, 930, 931, 932, 935, 936, 937, 938, 940, 942, 944, 945, 946, 947, 948, 949, 950, 951, 953, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 974, 975, 976, 978, 979, 981, 984, 985, 986, 988, 989, 990, 992, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1014, 1015, 1016, 1017, 1018, 1019, 1021, 1024, 1025, 1028, 1029, 1030, 1032, 1034, 1035, 1036, 1037, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1062, 1063, 1064, 1065, 1066, 1067, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1077, 1080, 1081, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1094, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\n",
      "Test indices:  [5, 17, 20, 22, 36, 39, 48, 49, 52, 55, 63, 64, 67, 75, 83, 102, 103, 109, 116, 133, 135, 137, 139, 146, 147, 153, 154, 161, 171, 172, 179, 189, 194, 197, 199, 202, 215, 216, 217, 222, 223, 241, 243, 244, 256, 258, 275, 287, 290, 296, 297, 299, 303, 318, 320, 332, 335, 340, 344, 353, 354, 375, 377, 378, 384, 386, 387, 393, 404, 408, 409, 413, 414, 425, 435, 439, 440, 443, 447, 448, 453, 454, 462, 464, 466, 469, 474, 475, 476, 482, 486, 501, 506, 507, 508, 510, 512, 513, 525, 531, 534, 543, 546, 549, 567, 569, 570, 577, 579, 581, 583, 595, 604, 605, 607, 608, 609, 610, 615, 616, 622, 637, 644, 648, 654, 657, 658, 660, 661, 662, 666, 680, 682, 684, 694, 695, 706, 711, 718, 728, 733, 741, 742, 744, 745, 752, 754, 757, 758, 762, 773, 776, 788, 803, 808, 812, 813, 814, 823, 828, 829, 830, 849, 852, 857, 860, 864, 866, 876, 878, 885, 886, 908, 915, 920, 926, 933, 934, 939, 941, 943, 952, 954, 973, 977, 980, 982, 983, 987, 991, 993, 994, 1005, 1013, 1020, 1022, 1023, 1026, 1027, 1031, 1033, 1038, 1054, 1061, 1068, 1076, 1078, 1079, 1082, 1091, 1095, 1096, 1105, 1106]\n",
      "Train Noise Shape:  (291, 64, 500)\n",
      "Train Noise avg Shape:  (64, 500)\n",
      "training data shape:  (901, 64, 500)\n",
      "training data shape:  (901, 64, 500)\n",
      "Subtraction Completed\n",
      "Train data (291, 64, 500)\n",
      "Submitting 98 tasks to the executor...\n",
      "Process 10852 working on start_time: 0\n",
      "Process 10852 working on start_time: 5\n",
      "Process 10852 working on start_time: 10\n",
      "Process 10852 working on start_time: 15\n",
      "Process 10852 working on start_time: 20\n",
      "Process 10852 working on start_time: 25\n",
      "Process 10852 working on start_time: 30\n",
      "Process 10852 working on start_time: 35\n",
      "Process 10852 working on start_time: 40\n",
      "Process 10852 working on start_time: 45\n",
      "Process 10852 working on start_time: 50\n",
      "Process 10852 working on start_time: 55\n",
      "Process 10852 working on start_time: 60\n",
      "Process 10852 working on start_time: 65\n",
      "Process 10852 working on start_time: 70\n",
      "Process 10852 working on start_time: 75\n",
      "Process 10852 working on start_time: 80\n",
      "Process 10852 working on start_time: 85\n",
      "Process 10852 working on start_time: 90\n",
      "Process 10852 working on start_time: 95\n",
      "Process 10852 working on start_time: 100\n",
      "Process 10852 working on start_time: 105\n",
      "Process 10852 working on start_time: 110\n",
      "Process 10852 working on start_time: 115\n",
      "Process 10852 working on start_time: 120\n",
      "Process 10852 working on start_time: 125\n",
      "Process 10852 working on start_time: 130\n",
      "Process 10852 working on start_time: 135\n",
      "Process 10852 working on start_time: 140\n",
      "Process 10852 working on start_time: 145\n",
      "Process 10852 working on start_time: 150\n",
      "Process 10852 working on start_time: 155\n",
      "Process 10852 working on start_time: 160\n",
      "Process 10852 working on start_time: 165\n",
      "Process 10852 working on start_time: 170\n",
      "Process 10852 working on start_time: 175\n",
      "Process 10852 working on start_time: 180\n",
      "Process 10852 working on start_time: 185\n",
      "Process 10852 working on start_time: 190\n",
      "Process 10852 working on start_time: 195\n",
      "Process 10852 working on start_time: 200\n",
      "Process 10852 working on start_time: 205\n",
      "Process 10852 working on start_time: 210\n",
      "Process 10852 working on start_time: 215\n",
      "Process 10852 working on start_time: 220\n",
      "Process 10852 working on start_time: 225\n",
      "Process 10852 working on start_time: 230\n",
      "Process 10852 working on start_time: 235\n",
      "Process 10852 working on start_time: 240\n",
      "Process 10852 working on start_time: 245\n",
      "Process 10852 working on start_time: 250\n",
      "Process 10852 working on start_time: 255\n",
      "Process 10852 working on start_time: 260\n",
      "Process 10852 working on start_time: 265\n",
      "Process 10852 working on start_time: 270\n",
      "Process 10852 working on start_time: 275\n",
      "Process 10852 working on start_time: 280\n",
      "Process 10852 working on start_time: 285\n",
      "Process 10852 working on start_time: 290\n",
      "Process 10852 working on start_time: 295\n",
      "Process 10852 working on start_time: 300\n",
      "Process 10852 working on start_time: 305\n",
      "Process 10852 working on start_time: 310\n",
      "Process 10852 working on start_time: 315\n",
      "Process 10852 working on start_time: 320\n",
      "Process 10852 working on start_time: 325\n",
      "Process 10852 working on start_time: 330\n",
      "Process 10852 working on start_time: 335\n",
      "Process 10852 working on start_time: 340\n",
      "Process 10852 working on start_time: 345\n",
      "Process 10852 working on start_time: 350\n",
      "Process 10852 working on start_time: 355\n",
      "Process 10852 working on start_time: 360\n",
      "Process 10852 working on start_time: 365\n",
      "Process 10852 working on start_time: 370\n",
      "Process 10852 working on start_time: 375\n",
      "Process 10852 working on start_time: 380\n",
      "Process 10852 working on start_time: 385\n",
      "Process 10852 working on start_time: 390\n",
      "Process 10852 working on start_time: 395\n",
      "Process 10852 working on start_time: 400\n",
      "Process 10852 working on start_time: 405\n",
      "Process 10852 working on start_time: 410\n",
      "Process 10852 working on start_time: 415\n",
      "Process 10852 working on start_time: 420\n",
      "Process 10852 working on start_time: 425\n",
      "Process 10852 working on start_time: 430\n",
      "Process 10852 working on start_time: 435\n",
      "Process 10852 working on start_time: 440\n",
      "Process 10852 working on start_time: 445\n",
      "Process 10852 working on start_time: 450\n",
      "Process 10852 working on start_time: 455\n",
      "Process 10852 working on start_time: 460\n",
      "Process 10852 working on start_time: 465\n",
      "Process 10852 working on start_time: 470\n",
      "Process 10852 working on start_time: 475\n",
      "Process 10852 working on start_time: 480\n",
      "Process 10852 working on start_time: 485\n",
      "Saved Run 0 Time: 83.59762380000029\n",
      "Starting split number 2 out of 2\n",
      "Starting moving_window_subsample...\n",
      "Removing Noise from Dataset\n",
      "Train indices:  [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 83, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 102, 103, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 131, 132, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 146, 147, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 164, 165, 166, 168, 169, 170, 171, 172, 174, 175, 177, 178, 179, 180, 181, 182, 183, 185, 186, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 255, 256, 258, 259, 260, 261, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 329, 330, 331, 332, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 419, 421, 423, 424, 425, 426, 430, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 464, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 480, 481, 482, 483, 485, 486, 487, 488, 490, 491, 492, 493, 494, 496, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 510, 511, 512, 513, 515, 516, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 533, 534, 535, 536, 537, 538, 540, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 567, 568, 569, 570, 571, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 587, 588, 589, 590, 591, 593, 594, 595, 599, 602, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 622, 623, 624, 625, 627, 628, 629, 631, 632, 633, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 656, 657, 658, 659, 660, 661, 662, 663, 665, 666, 667, 668, 669, 670, 671, 672, 675, 676, 678, 679, 680, 681, 682, 683, 684, 689, 690, 691, 694, 695, 697, 698, 700, 702, 703, 704, 705, 706, 708, 709, 711, 712, 713, 714, 715, 716, 718, 719, 720, 721, 722, 723, 724, 726, 727, 728, 730, 733, 734, 735, 736, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 750, 751, 752, 753, 754, 756, 757, 758, 759, 760, 762, 763, 764, 765, 766, 767, 768, 769, 770, 773, 774, 775, 776, 777, 778, 779, 780, 785, 786, 787, 788, 789, 790, 793, 794, 797, 798, 800, 802, 803, 804, 805, 806, 807, 808, 809, 811, 812, 813, 814, 815, 817, 818, 819, 821, 823, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 839, 840, 841, 842, 843, 845, 846, 848, 849, 852, 855, 857, 858, 859, 860, 861, 862, 864, 865, 866, 867, 868, 869, 870, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 884, 885, 886, 887, 888, 889, 890, 891, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 913, 914, 915, 917, 918, 919, 920, 921, 923, 924, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 938, 939, 940, 941, 943, 944, 945, 946, 947, 948, 949, 951, 952, 953, 954, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 967, 968, 969, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1001, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1061, 1062, 1063, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1094, 1095, 1096, 1097, 1098, 1100, 1101, 1102, 1103, 1105, 1106, 1109, 1111, 1112, 1113, 1114]\n",
      "Test indices:  [2, 3, 4, 16, 24, 25, 34, 43, 61, 77, 81, 82, 84, 86, 87, 97, 101, 104, 108, 112, 127, 130, 138, 140, 145, 148, 151, 160, 162, 167, 173, 176, 184, 187, 195, 209, 228, 229, 234, 240, 248, 254, 257, 262, 266, 284, 288, 292, 305, 308, 314, 322, 327, 328, 333, 334, 341, 347, 360, 372, 374, 385, 389, 392, 411, 418, 420, 422, 427, 428, 429, 431, 446, 449, 459, 463, 465, 470, 479, 484, 489, 495, 497, 498, 509, 514, 517, 529, 539, 541, 566, 572, 575, 584, 585, 586, 592, 596, 597, 598, 600, 601, 603, 606, 621, 626, 630, 634, 635, 636, 655, 664, 673, 674, 677, 685, 686, 687, 688, 692, 693, 696, 699, 701, 707, 710, 717, 725, 729, 731, 732, 737, 738, 749, 755, 761, 771, 772, 781, 782, 783, 784, 791, 792, 795, 796, 799, 801, 810, 816, 820, 822, 824, 831, 837, 838, 844, 847, 850, 851, 853, 854, 856, 863, 871, 882, 883, 892, 893, 911, 912, 916, 922, 925, 931, 937, 942, 950, 955, 966, 970, 984, 1000, 1002, 1014, 1029, 1050, 1059, 1060, 1064, 1072, 1073, 1074, 1085, 1093, 1099, 1104, 1107, 1108, 1110]\n",
      "Train Noise Shape:  (316, 64, 500)\n",
      "Train Noise avg Shape:  (64, 500)\n",
      "training data shape:  (915, 64, 500)\n",
      "training data shape:  (915, 64, 500)\n",
      "Subtraction Completed\n",
      "Train data (316, 64, 500)\n",
      "Submitting 98 tasks to the executor...\n",
      "Process 10852 working on start_time: 0\n",
      "Process 10852 working on start_time: 5\n",
      "Process 10852 working on start_time: 10\n",
      "Process 10852 working on start_time: 15\n",
      "Process 10852 working on start_time: 20\n",
      "Process 10852 working on start_time: 25\n",
      "Process 10852 working on start_time: 30\n",
      "Process 10852 working on start_time: 35\n",
      "Process 10852 working on start_time: 40\n",
      "Process 10852 working on start_time: 45\n",
      "Process 10852 working on start_time: 50\n",
      "Process 10852 working on start_time: 55\n",
      "Process 10852 working on start_time: 60\n",
      "Process 10852 working on start_time: 65\n",
      "Process 10852 working on start_time: 70\n",
      "Process 10852 working on start_time: 75\n",
      "Process 10852 working on start_time: 80\n",
      "Process 10852 working on start_time: 85\n",
      "Process 10852 working on start_time: 90\n",
      "Process 10852 working on start_time: 95\n",
      "Process 10852 working on start_time: 100\n",
      "Process 10852 working on start_time: 105\n",
      "Process 10852 working on start_time: 110\n",
      "Process 10852 working on start_time: 115\n",
      "Process 10852 working on start_time: 120\n",
      "Process 10852 working on start_time: 125\n",
      "Process 10852 working on start_time: 130\n",
      "Process 10852 working on start_time: 135\n",
      "Process 10852 working on start_time: 140\n",
      "Process 10852 working on start_time: 145\n",
      "Process 10852 working on start_time: 150\n",
      "Process 10852 working on start_time: 155\n",
      "Process 10852 working on start_time: 160\n",
      "Process 10852 working on start_time: 165\n",
      "Process 10852 working on start_time: 170\n",
      "Process 10852 working on start_time: 175\n",
      "Process 10852 working on start_time: 180\n",
      "Process 10852 working on start_time: 185\n",
      "Process 10852 working on start_time: 190\n",
      "Process 10852 working on start_time: 195\n",
      "Process 10852 working on start_time: 200\n",
      "Process 10852 working on start_time: 205\n",
      "Process 10852 working on start_time: 210\n",
      "Process 10852 working on start_time: 215\n",
      "Process 10852 working on start_time: 220\n",
      "Process 10852 working on start_time: 225\n",
      "Process 10852 working on start_time: 230\n",
      "Process 10852 working on start_time: 235\n",
      "Process 10852 working on start_time: 240\n",
      "Process 10852 working on start_time: 245\n",
      "Process 10852 working on start_time: 250\n",
      "Process 10852 working on start_time: 255\n",
      "Process 10852 working on start_time: 260\n",
      "Process 10852 working on start_time: 265\n",
      "Process 10852 working on start_time: 270\n",
      "Process 10852 working on start_time: 275\n",
      "Process 10852 working on start_time: 280\n",
      "Process 10852 working on start_time: 285\n",
      "Process 10852 working on start_time: 290\n",
      "Process 10852 working on start_time: 295\n",
      "Process 10852 working on start_time: 300\n",
      "Process 10852 working on start_time: 305\n",
      "Process 10852 working on start_time: 310\n",
      "Process 10852 working on start_time: 315\n",
      "Process 10852 working on start_time: 320\n",
      "Process 10852 working on start_time: 325\n",
      "Process 10852 working on start_time: 330\n",
      "Process 10852 working on start_time: 335\n",
      "Process 10852 working on start_time: 340\n",
      "Process 10852 working on start_time: 345\n",
      "Process 10852 working on start_time: 350\n",
      "Process 10852 working on start_time: 355\n",
      "Process 10852 working on start_time: 360\n",
      "Process 10852 working on start_time: 365\n",
      "Process 10852 working on start_time: 370\n",
      "Process 10852 working on start_time: 375\n",
      "Process 10852 working on start_time: 380\n",
      "Process 10852 working on start_time: 385\n",
      "Process 10852 working on start_time: 390\n",
      "Process 10852 working on start_time: 395\n",
      "Process 10852 working on start_time: 400\n",
      "Process 10852 working on start_time: 405\n",
      "Process 10852 working on start_time: 410\n",
      "Process 10852 working on start_time: 415\n",
      "Process 10852 working on start_time: 420\n",
      "Process 10852 working on start_time: 425\n",
      "Process 10852 working on start_time: 430\n",
      "Process 10852 working on start_time: 435\n",
      "Process 10852 working on start_time: 440\n",
      "Process 10852 working on start_time: 445\n",
      "Process 10852 working on start_time: 450\n",
      "Process 10852 working on start_time: 455\n",
      "Process 10852 working on start_time: 460\n",
      "Process 10852 working on start_time: 465\n",
      "Process 10852 working on start_time: 470\n",
      "Process 10852 working on start_time: 475\n",
      "Process 10852 working on start_time: 480\n",
      "Process 10852 working on start_time: 485\n",
      "Saved Run 1 Time: 81.1764819\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_pipeline_for_subject(2, all_data)\n",
    "\n",
    "\n",
    "#for sub in np.unique(all_data.subject_id):\n",
    "#    run_pipeline_for_subject(sub, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sub_2_reed_LogisticRegression_IB_TG_NoiseRemoval_RoundRobin_round_0_10_step_5_2fold_8avg_2024-09-22_16h-31m-48s.pkl\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mint\u001b[39m(i), pickle_files[i])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pickle_files[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 5\u001b[0m         all_splits\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(all_splits)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      8\u001b[0m print_grandavg(all_splits)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_splits = []\n",
    "for i in range(len(pickle_files)):\n",
    "    print(int(i), pickle_files[i])\n",
    "    with open(pickle_files[i], 'rb') as file:\n",
    "        all_splits.append(pickle.load(file))\n",
    "\n",
    "print(np.array(all_splits).shape)\n",
    "print_grandavg(all_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sub_2_reed_LogisticRegression_IB_TG_NoiseRemoval_RoundRobin_round_0_10_step_5_2fold_8avg_2024-09-22_16h-31m-48s.pkl', 'Sub_2_reed_LogisticRegression_IB_TG_NoiseRemoval_RoundRobin_round_0_10_step_5_2fold_8avg_2024-09-22_16h-40m-24s.pkl', 'Sub_2_reed_LogisticRegression_IB_TG_NoiseRemoval_RoundRobin_round_0_10_step_5_2fold_8avg_2024-09-22_16h-50m-32s.pkl', 'Sub_2_reed_LogisticRegression_IB_TG_NoiseRemoval_RoundRobin_round_0_10_step_5_2fold_8avg_2024-09-22_16h-59m-41s.pkl', 'Sub_2_reed_LogisticRegression_IB_TG_NoiseRemoval_RoundRobin_round_0_10_step_5_5fold_8avg_2024-09-22_21h-03m-35s.pkl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_files = [file for file in os.listdir(os.getcwd()) if file.endswith(\".pkl\") and 'IB_TG_NoiseRemoval_RoundRobin_round_0' in file]\n",
    "print(pickle_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_files = [file for file in os.listdir(os.getcwd()) if file.endswith(\".pkl\") and 'IB_TG_NoiseRemoval_RoundRobin_round_1' in file]\n",
    "print(pickle_files)\n",
    "\n",
    "all_splits = []\n",
    "for i in range(len(pickle_files)):\n",
    "    print(int(i), pickle_files[i])\n",
    "    with open(pickle_files[i], 'rb') as file:\n",
    "        all_splits.append(pickle.load(file))\n",
    "\n",
    "print(np.array(all_splits).shape)\n",
    "print_grandavg(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grandavg_alone(avg_scores,labels,splits):\n",
    "\n",
    "    bootstrap_aucs = []\n",
    "\n",
    "    num_bootstrap_samples  = 1000\n",
    "    bootstrap_samples = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "    print('avg scores shape',avg_scores.shape)\n",
    "    \n",
    "    # Create a line plot\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    clrs = sns.color_palette(\"husl\", 5)\n",
    "    clrs = ['b','g','orange']\n",
    "    \n",
    "    print(labels)\n",
    "    \n",
    "    #data_length = splits[0][0].subject.data.shape[2]\n",
    "    window_size = splits.window\n",
    "    step_size = splits.step\n",
    "\n",
    "    data_length = (avg_scores.shape[-1] - 1) * step_size + window_size\n",
    "    print(data_length)\n",
    "\n",
    "    num_of_tests = avg_scores.shape[1] # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    num_of_rounds = 1 # CHANGE TO FIT THE NUMBER OF TRAINS AND TESTS\n",
    "    print('num of rounds', 1)\n",
    "    print('num of tests', avg_scores.shape[1])\n",
    "\n",
    "    fig = plt.figure(figsize=(num_of_tests*4,num_of_rounds*4))\n",
    "\n",
    "    # NEEDS TO BE CORRECTED FOR 512 collections\n",
    "    time_windows = [i for i in range(0, (data_length * 2) - (window_size * 2), step_size * 2)]\n",
    "\n",
    "    colors = [(0, 1, 0), (0, 0, 1), (1, 1, 1), (1, 0, 0), (1, 1, 0)]  # Blue, White, Red, yellow\n",
    "    cmap = LinearSegmentedColormap.from_list(\"Custom_RdBu\", colors)\n",
    "    \n",
    "    # Define custom smoothing kernel (ignore diagonals)\n",
    "    kernel = np.array([[1, 1, 0],\n",
    "                    [1, 2, 1],\n",
    "                    [0, 1, 1]])\n",
    "    \n",
    "    # Plot the averages with error bars\n",
    "\n",
    "    count = 1\n",
    "    print(avg_scores.shape)\n",
    "    \n",
    "    for i in range(num_of_rounds):\n",
    "        for l in range(num_of_tests):\n",
    "            print('l', l)\n",
    "            ax = plt.subplot(num_of_rounds,num_of_tests,count)\n",
    "            \n",
    "            ax.set_title(labels[0][l], loc='center', fontsize=16, y=1, x = .5)\n",
    "\n",
    "            heatmap_data = avg_scores[:,l,:]\n",
    "            print(min_cis.shape)\n",
    "            #min_cis_data = min_cis[:,l,:]\n",
    "            #max_cis_data = max_cis[:,l,:]\n",
    "\n",
    "             # Normalize kernel\n",
    "            kernel = kernel / np.sum(kernel)\n",
    "\n",
    "            # Perform 2D convolution with the custom kernel\n",
    "            smoothed_data    = convolve2d(heatmap_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            #smoothed_min_cis = convolve2d(min_cis_data, kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "            #smoothed_max_cis = convolve2d(max_cis_data, kernel, mode='same', boundary='fill', fillvalue=0) \n",
    "\n",
    "            #pos_data = np.where(smoothed_min_cis > 0.5, 1, 0)\n",
    "            #neg_data = np.where(smoothed_max_cis < 0.5, 1, 0)\n",
    "\n",
    "            #final_data = pos_data + neg_data\n",
    "\n",
    "            #smoothed_data = smoothed_data * final_data\n",
    "            # SHOW AND HIDE CBAR\n",
    "            cbar_bool = True\n",
    "            ax2 = sns.heatmap(smoothed_data, vmin = 0, vmax = 1, xticklabels = time_windows, yticklabels = time_windows, cmap = cmap,annot=False, square=True, cbar = True)\n",
    "            original_position = ax.get_position()\n",
    "            if count != 1:\n",
    "                cbar_bool = False\n",
    "                #sns.cbar = cbar_bool\n",
    "                cbar = ax.collections[0].colorbar\n",
    "                \n",
    "                cbar.remove()\n",
    "                ax.set_position(original_position)\n",
    "            # HIDE TICKS\n",
    "            show_every = 12\n",
    "            for i, label in enumerate(plt.gca().xaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().xaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            for i, label in enumerate(plt.gca().yaxis.get_ticklabels()):\n",
    "                if i % show_every != 0:\n",
    "                    label.set_visible(False)\n",
    "                    plt.gca().yaxis.get_major_ticks()[i].set_visible(False)\n",
    "\n",
    "            # Plot your heatmap here, and set the x-axis ticks to the new time windows\n",
    "            #plt.xticks(range(len(new_time_windows)), new_time_windows)\n",
    "            #plt.contour(final_data, colors='black', linestyles='dotted', linewidths=.5, alpha = 0.5)\n",
    "            #sns.set_palette(\"RdBu\")\n",
    "            count = count + 1\n",
    "            start_index = labels[l].find(\"Train\")\n",
    "            end_index = labels[l].find(\"Test\")\n",
    "            #stim = find_word_stim(title)\n",
    "            between_train_and_iteration = labels[l][start_index:end_index].strip()\n",
    "            new_title = f\"Average AUCs, {between_train_and_iteration}\"\n",
    "            \n",
    "            plt.gca().invert_yaxis()\n",
    "            # Show the plot\n",
    "            # PLOT DIAGNOAL\n",
    "            plt.plot([0, avg_scores.shape[2]], [0, avg_scores.shape[0]], color='black', linestyle='-', linewidth=.5, alpha = 0.3)\n",
    "            plt.grid(False)\n",
    "    \n",
    "    # Creates a new empty subplot emcompassing the whole figure (111)\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axis\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    # Adds the labels to this big new subplot\n",
    "    plt.xlabel('Test', fontsize=18)\n",
    "    plt.ylabel('Train', fontsize=18)\n",
    "\n",
    "    # Annotate the plot with subject information\n",
    "    sub_info = splits.subject\n",
    "    subtitle = f'Grand Avg Round {splits.round_number} Lab {sub_info.lab}, Model {splits.model.__class__.__name__}, Average of {splits.average} Window length {window_size}, Step size {step_size}'\n",
    "\n",
    "# Add a square at the top right of the plot containing the subject information\n",
    "#plt.text(0.5, 1.1, info_text, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "# legs = ''\n",
    "    \n",
    "    plt.suptitle('\\n'+ new_title, fontsize=16,y=1.10)\n",
    "    plt.figtext(0.5, 0.95, subtitle, ha='center', va='center')\n",
    "    #filename = f'{models_name[i]}_{mods}_{window_size}_step_{step_size}_{formatted_datetime}_{num_splits}fold_{avg_num}avg'\n",
    "    plt.savefig( subtitle + new_title + 'subplots' + '.png', format='png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
